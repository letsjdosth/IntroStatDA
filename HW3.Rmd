---
title: "Homework Assignment 3"
author: "Seokjun Choi"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

### \noindent Problem1.
**Albert and Rizzo, Chapter 4, problem 3**

**4.3 (Speed and stopping distance (continued)).**

**a. Construct a residual plot for the linear fit by typing**

**plot(cars\$speed, fit.linear\$residual)**


**b. Add a blue, thick (lwd=3) horizontal line to the residual plot using the**
**abline function.**


**c. There are two large positive residuals in this graph. By two applications**
**of the text function, label each residual using the label “POS” in blue.**


**d. Label the one large negative residual in the graph with the label “NEG”**
**in red.**

```{r}
#a
library(datasets)
data(cars)
cars_fit_linear <- lm(dist ~ speed, data = cars)
plot(cars$speed, cars_fit_linear$residual)

#b
abline(h = 0, lwd = 3, col = "blue")

#c
large_pos_res_idx <- which(cars_fit_linear$residual > 40)
text(cars$speed[large_pos_res_idx], 
  cars_fit_linear$residuals[large_pos_res_idx], label = "POS",
  col = "blue", pos = 1)

#d
large_neg_res_idx <- which.min(cars_fit_linear$residual)
text(cars$speed[large_neg_res_idx], 
  cars_fit_linear$residuals[large_neg_res_idx], label = "NEG",
  col = "red", pos = 3)
```

**e. Use the identify function to find the row numbers of two observations**
**that have residuals close to zero.**

Because we cannot use the 'identify' function during the R markdown compile process,
I will use which function instead.

```{r}
abs_resid <- abs(cars_fit_linear$residuals)
which(abs_resid <= sort(abs_resid)[2])
cars_fit_linear$residuals[which(abs_resid <= sort(abs_resid)[2])]
```

Points at the 17th and the 18th rows are close to zero.


### \noindent Problem2.

**Albert and Rizzo, Chapter 4, problem 4**

**4.4 (Multiple graphs). The dataset mtcars contains measurements of fuel**
**consumption (variable mpg) and other design and performance characteristics**
**for a group of 32 automobiles. Using the mfrow argument to par, construct**
**scatterplots of each of the four variables disp (displacement), wt (weight), hp**
**(horsepower), drat (rear axle ratio) with mileage (mpg) on a two by two array.**
**Comparing the four graphs, which variable among displacement, weight,**
**horsepower, and rear axle ratio has the strongest relationship with mileage?**

```{r}
data(mtcars)
names(mtcars)
```

```{r}
par(mfrow = c(2, 2))
plot(mtcars$disp, mtcars$mpg)
plot(mtcars$wt, mtcars$mpg)
plot(mtcars$hp, mtcars$mpg)
plot(mtcars$drat, mtcars$mpg)
cor(mtcars)["mpg", c("disp", "wt", "hp", "drat")]
```

Not only graphs but also correlation values show that
'wt' (weight) has the strongest relationship with 'mpg' (mileage) among the four variables.


### \noindent Problem3.

**Albert and Rizzo, Chapter 4, problem 6**

**4.6 (Drawing beta density curves). Suppose one is interesting in displaying**
**three members of the beta family of curves, where the beta density with**
**shape parameters a and b (denoted by Beta(a, b)) is given by**
\[f(y) = \frac{1}{B(a,b)} y^{a-1} (1-y)^{b-1}, 0< y < 1\]
**One can draw a single beta density, say with shape parameters $a = 5$ and**
**$b = 2$, using the curve function:**

**curve(dbeta(x, 5, 2), from = 0, to = 1)**

**a. Use three applications of the curve function to display the Beta(2, 6),**
**Beta(4, 4), and Beta(6, 2) densities on the same plot. (The curve function**
**with the add=TRUE argument will add the curve to the current plot.)**

**b. Use the following R command to title the plot with the equation of the beta density.**

**title(expression(f(y)==frac(1,B(a,b))*y^{a-1}*(1-y)^{b-1}))**

**c. Using the text function, label each of the beta curves with**
**the corresponding values of the shape parameters a and b.**

```{r}
#a
curve(dbeta(x, 2, 6), from = 0, to = 1, xlab = "y", ylab = "density")
curve(dbeta(x, 4, 4), add = TRUE)
curve(dbeta(x, 6, 2), add = TRUE)

#b
title(expression(f(y) == frac(1, B(a,b))*y^{a-1}*(1-y)^{b-1}))

#c
text(0.01, 2.5, pos = 4, label = "a=2, b=6")
text(0.5, 2.25, label = "a=4, b=4")
text(0.99, 2.5, pos = 2, label = "a=6, b=2")
```

**d. Redraw the graph using different colors or line types for the three beta**
**density curves.**

**e. Instead of using the text function, add a legend to the graph that shows**
**the color or line type for each of the beta density curves.**

```{r}
#d
curve(dbeta(x, 2, 6), from = 0, to = 1, xlab = "y", ylab = "density")
curve(dbeta(x, 4, 4), add = TRUE, col = "red")
curve(dbeta(x, 6, 2), add = TRUE, col = "blue")
title(expression(f(y) == frac(1, B(a,b))*y^{a-1}*(1-y)^{b-1}))

#e
legend("top", lty = c(1,1,1), col = c("black", "red", "blue"),
  legend = c("a=2, b=6", "a=4, b=4", "a=6, b=2"), inset=0.02)
```


### \noindent Problem4.

**Albert and Rizzo, Chapter 6, problem 3 (a and b)**

**6.3 (Ages of marathoners, continued). From the information in the 2005**
**report, one may believe that men marathoners tend to be older than women**
**marathons.**

```{r}
marathoners <- read.csv("Rx-data/nyc-marathon.csv")
head(marathoners)
marathoners$Gender <- factor(marathoners$Gender)

```



**a. Use the t.test function to construct a test of the hypothesis that the mean**
**ages of women and men marathoners are equal against the alternative**
**hypothesis that the mean age of men is larger.**

First, let's do EDA.
```{r}
par(mfrow = c(1, 3))
plot(marathoners$Gender, marathoners$Age, xlab = "Gender", ylab = "Age")
hist(marathoners$Age[which(marathoners$Gender == "female")],
  xlab = "female", xlim = c(20, 75), breaks = 50, main = "")
hist(marathoners$Age[which(marathoners$Gender == "male")],
  xlab = "male", xlim = c(20, 75), breaks = 50, main = "")

par(mfrow = c(1, 2))
qqnorm(marathoners$Age[which(marathoners$Gender == "female")],
  main="normal Q-Q plot (female)")
qqnorm(marathoners$Age[which(marathoners$Gender == "male")],
  main="normal Q-Q plot (male)")


c(mean(marathoners$Age[which(marathoners$Gender == "female")]),
  mean(marathoners$Age[which(marathoners$Gender == "male")]))

c(var(marathoners$Age[which(marathoners$Gender == "female")]),
  var(marathoners$Age[which(marathoners$Gender == "male")]))
```

Two variances seem different, but let's conduct a test for variance 
to check whether they are statistically different or not.

```{r}
var.test(Age ~ Gender, data = marathoners)
```

The p-value is 0.05602. If we use a significance level of 0.05, we cannot reject the H0 technically.
However, the p-value is so close to 0.05. So, I will do t-tests of both two versions (equal var / non-equal var).

First, assume that two gender groups have non-equal variances of their ages.

H0 : mean of women = mean of men
H1 : mean of women < mean of men, or mean of women - mean of men < 0

```{r}
marathoners_ttest<- t.test(Age ~ Gender, data = marathoners, alternative = "less")
marathoners_ttest
```

The p-value is 0.007443. If we use a significance level higher than 0.007443 (like 0.05 or 0.01,)
we can reject H0. Thus, Two gender groups don't have the same mean value of age.



Next, suppose that two gender groups have the same variance.
The hypothesis is the same as the above case.

```{r}
marathoners_ttest_equalvar<- t.test(Age ~ Gender, data = marathoners, 
  alternative = "less", var.equal = TRUE)
marathoners_ttest_equalvar
```

The t-value, the degrees of freedom, and p-value are different from the above case,
but anyway, the p-value is 0.009495. This is smaller than common significance levels like 0.05 or 0.01.
We can reject H0 under 0.05 or 0.01 significance levels.
Thus, Two gender groups don't have the same mean value of age.


**b. Construct a 90% interval estimate for the difference in mean ages of men**
**and women marathoners.**

```{r}
t.test(Age ~ Gender, data = marathoners, alternative = "less",
  conf.level = 0.9)$conf.int
t.test(Age ~ Gender, data = marathoners, alternative = "less",
  var.equal = TRUE, conf.level = 0.9)$conf.int
```
The 90% confidence interval is $(\infty, -1.420086)$ under the different variances assumption,
and $(\infty, 1.359311)$ under the equal-variance assumption.



### \noindent Problem5.

**Albert and Rizzo, Chapter 6, problem 4**

**6.4 (Measuring the length of a string). An experiment was performed**
**in an introductory statistics class to illustrate the concept of measurement**
**bias. The instructor held up a string in front of the class and each student**
**guessed at the string's length. The following are the measurements from the**
**24 students (in inches).**

22 18 27 23 24 15 26 22 24 25 24 18
18 26 20 24 27 16 30 22 17 18 22 26

**a. Use the scan function to enter these measurements into R.**

The 'scan' function does not work in the R markdown compiling process.
Thus, I make the data vector directly instead.

```{r}
stringlen <- c(22, 18, 27, 23, 24, 15, 26, 22, 24, 25, 24, 18,
18, 26, 20, 24, 27, 16, 30, 22, 17, 18, 22, 26)

# EDA
par(mfrow = c(1, 3))
boxplot(stringlen, main = "gueessed string-length")
hist(stringlen, breaks = 10, main = "", xlab = "")
qqnorm(stringlen)
qqline(stringlen)
summary(stringlen)
```

Note that the mean value is 22.25.

**b. The true length of the string was 26 inches. Assuming that this sample of**
**measurements represents a random sample from a population of student**
**measurements, use the t.test function to test the hypothesis that the**
**mean measurement $\mu$ is different from 26 inches.**

Hypotheses are

H0 : $\mu = 26$

H1 : $\mu \neq 26$

```{r}
stringlen_ttest <- t.test(stringlen, alternative = "two.sided", mu = 26)
stringlen_ttest
```

The T statistic is -4.6148, which follows the student-t distribution with the degree of freedom 23 under H0.
The p-value is 0.0001216, so we can reject H0 with a significance level higher than 0.0001216, like 0.05 or 0.01.
Thus, under these levels, $\mu$ is not 26.


**c. Use the t.test function to find a 90% confidence interval for the population mean $\mu$.**
```{r}
t.test(stringlen, alternative = "two.sided", mu = 26, conf.level = 0.9)$conf.int
```

The 90% confidence interval of $\mu$ is $(20.8573, 23.6427)$.

**d. The t-test procedure assumes the sample is from a population that is**
**normally distributed. Construct a normal probability plot of the measurements**
**and decide if the assumption of normality is reasonable.**

In the EDA at the problem (a), I already have drawn the normal probability plot (or normal Q-Q plot),
but I repeat it here.

```{r}
par(mfrow = c(1, 1))
qqnorm(stringlen)
qqline(stringlen)
```

Unfortunately, the Q-Q plot's points are not on a line.
This can be confirmed again in the histogram result of (a).
So, the normality assumption does not seem reasonable in this case.



### \noindent Problem6.

**Albert and Rizzo, Chapter 6, problem 6 (a and b)**

**6.6 (Comparing Etruscan and modern Italian skulls). Researchers**
**were interested if ancient Etruscans were native to Italy.**
**The dataset“Etruscan-Italian.txt” contains the skull measurements**
**from a group of Etruscans and modern Italians.**
**There are two relevant variables in the dataset: x is the skull**
**measurement and group is the type of skull.**

```{r}
etru_ital <- read.table("Rx-data/Etruscan-Italian.txt", header = TRUE)
etru_ital$group <- factor(etru_ital$group)
names(etru_ital) <- c("skull", "group")
head(etru_ital)
```

```{r}
par(mfrow = c(1, 3))
plot(etru_ital$group, etru_ital$skull, xlab = "group", ylab = "skull", main = "skull")
hist(etru_ital$skull[which(etru_ital$group == "Etruscan")],
  xlab = "Etruscan", xlim = c(110, 165), breaks = 30, main = "Etruscan")
hist(etru_ital$skull[which(etru_ital$group == "Italian")],
  xlab = "Italian", xlim = c(110, 165), breaks = 30, main = "Italian")

par(mfrow = c(1, 2))
qqnorm(etru_ital$skull[which(etru_ital$group == "Etruscan")],
  main="normal Q-Q plot (Etruscan)")
qqline(etru_ital$skull[which(etru_ital$group == "Etruscan")])
qqnorm(etru_ital$skull[which(etru_ital$group == "Italian")],
  main = "normal Q-Q plot (Italian)")
qqline(etru_ital$skull[which(etru_ital$group == "Italian")])

c(mean(etru_ital$skull[which(etru_ital$group == "Etruscan")]),
  mean(etru_ital$skull[which(etru_ital$group == "Italian")]))

c(var(etru_ital$skull[which(etru_ital$group == "Etruscan")]),
  var(etru_ital$skull[which(etru_ital$group == "Italian")]))
```

The boxplot shows that the two centers of both groups are different, but spread is similar.
These facts are shown by group means and variances.
When we look at the histograms and the Q-Q plots, 
we can think that these two groups are normaly distributed.


**a. Assuming that the data represent independent samples from normal distributions,**
**use the t.test function to test the hypothesis that the mean**
**Etruscan skull measurement $\mu_E$ is equal to the mean Italian skull measurement $\mu_I$ .**

First, let's test whether variances are equal or not.

H0 : two groups' variances are equal

H1 : two groups' variances are different

```{r}
var.test(skull ~ group, data = etru_ital)
```

The p-value (0.7503) is quite high. So, we cannot reject the H0 under ordinary significance levels like 0.01 or 0.05.
Thus, let's conduct the t-test under the equal-variance assumption.

H0 : $\mu_E = \mu_I$

H1 : $\mu_E \neq \mu_I$

```{r}
etru_ital_ttest <- t.test(skull ~ group, data = etru_ital, var.equal = TRUE)
etru_ital_ttest
```

The T statistic value is 11.925, which follows the student-t distribution with df 152 under H0.
The p-value is very small (so close to 0). So we can reject H0.
Thus, we can conclude that their means are different.


**b. Use the t.test function to construct a 95% interval estimate**
**for the difference in means $\mu_E - \mu_I$.**
**c. Use the two-sample Wilcoxon procedure implemented in the function**
**wilcox.test to find an alternative 95% interval estimate for the difference**
**$\mu_E - \mu_I$.**

From the above R t-test result object, we can get the 95% interval for the difference.

```{r}
etru_ital_ttest$conf.int
```

The interval is $(9.45365, 13.20825)$.


### \noindent Problem7.

**Albert and Rizzo, Chapter 6, problem 7**

**6.7 (President's heights). In Example 1.2, the height of the election winner**
**and loser were collected for the U.S. Presidential elections of 1948 through 2008.**
**Suppose you are interested in testing the hypothesis that the mean**
**height of the election winner is equal to the mean height of the election loser.**
**Assuming that this data represent paired data from a hypothetical population**
**of elections, use the t.test function to test this hypothesis. Interpret the**
**results of this test.**

First, let's enter data.

```{r}
year <- seq(from=2008, to=1948, by=-4)
height_winner <- c(185, 182, 182, 188, 188, 188, 185, 185, 177,
  182, 182, 193, 183, 179, 179, 175)
height_opponent <- c(175, 193, 185, 187, 188, 173, 180, 177, 183,
  185, 180, 180, 182, 178, 178, 173)
height_difference <- height_winner - height_opponent
president_height <- data.frame(year,
  height_winner, height_opponent, height_difference)
```

Next, let's see the differences by our eyes.

```{r}
# EDA
par(mfrow = c(1, 3))
boxplot(height_difference, main = "height difference")
hist(height_difference, breaks = 5, main = "height difference",
  xlab = "", ylab = "winner - opponent")
qqnorm(height_difference)
qqline(height_difference)
summary(height_difference)
```

Samples are nearly symmetric and have a mode at the center (near 0), but they don't seem normally distributed.
The box in the boxplot contains 0. Not only that, median and mean values are close to 0.

Next is the paired t-test. Suppose that the difference follows the normal distribution 
(even if I'm a little skeptical about this assumption).

H0: height difference between winner and opponent is 0.

H1: height difference between winner and opponent is not 0.

```{r}
president_height_ttest <- t.test(height_difference, alternative = "two.sided")
president_height_ttest

president_height_ttest2 <- t.test(height_winner, height_opponent, 
  data = president_height, alternative = "two.sided", paired = TRUE)
president_height_ttest2
```

Basically, the above two t-tests are equivalent.
The T statistic is 1.3279, and it follows the student-t distribution with df=15 under H0.
The p-value is 0.2041, so we cannot reject H0 with significance levels lower than 0.2041, like 0.05 or 0.01.
Thus, we cannot conclude that the height difference between winner and opponent is not 0.



### \noindent Problem8.

**Albert and Rizzo, Chapter 7, problem 5**

**7.5 (Hubble's Law). In 1929 Edwin Hubble investigated the relationship**
**between distance and velocity of celestial objects.**
**Knowledge of this relationship might give clues as to how the universe was**
**formed and what may happen in the future.**
**Hubble's Law is**
\[Recession Velocity = H_0 \times Distance \]
**where $H_0$ is Hubble's constant. This model is a straight line through the origin**
**with slope $H_0$. Data that Hubble used to estimate the constant $H_0$ are given**
**on the DASL web at http://lib.stat.cmu.edu/DASL/Datafiles/Hubble.html.**
**Use the data to estimate Hubble's constant by simple linear regression.**

The link does not work. So, I will import the dataset from the 'gamair' package.

```{r}
library(gamair)
data(hubble)
# ?hubble
# Galaxy : A (factor) label identifying the galaxy.
# y : The galaxy's relative velocity in kilometres per second.
# x : The galaxy's distance in Mega parsecs. 1 parsec is 3.09e13 km.

names(hubble)[2:3] <- c("velocity", "distance")
head(hubble)
plot(velocity ~ distance, data = hubble, main = "hubble data")
```

To estimate the $H_0$ which is a slope of the fitted line, I'll use linear regression models.
Note that Hubble's law does not have an intercept term, so our model becomes

```{r}
hubble_lm_fit <- lm(velocity ~ 0 + distance, data = hubble)
summary(hubble_lm_fit)
```

We can reject the H0 of the t-test (H0: $H_0=0$, H1: $H_0 \neq 0$) under confidencee level 0.05 or 0.01,
so the coefficient is not 0.

The estimated coefficient (Hubble constant) is 76.58. In other words,
\[\hat{velocity} = 76.58 * distance\]

```{r}
par(mfrow = c(1, 1))
plot(velocity ~ distance, data = hubble, main = "hubble data")
abline(hubble_lm_fit)
text(hubble$distance[15], hubble$velocity[15], label = "15th", pos = 1)
text(hubble$distance[3], hubble$velocity[3], label = "3rd", pos = 1)
```

The line seems good, although there are some data points that are far from the line. (For example, 3rd and 15th points.)

Here are the residual plot and the q-q plot.

```{r}
par(mfrow = c(2, 2))
plot(hubble_lm_fit)
```

Except for the two points I mentioned above, residuals have no pattern, and all data points are on the normal q-q line.
If we remove the two points and fit the same model, we might get better results.

Below is the result excluding those two points. I'll just give you the results without explanation.

```{r}
hubble_without2 <- hubble[-c(3, 15),]
hubble_without2_lm_fit <- lm(velocity ~ 0 + distance, data = hubble_without2)
summary(hubble_without2_lm_fit)

par(mfrow = c(1, 1))
plot(velocity ~ distance, data = hubble_without2, 
  main = "hubble data (without 2 outliers)")
abline(hubble_without2_lm_fit)

par(mfrow = c(2, 2))
plot(hubble_without2_lm_fit)
```

The new estimated constant is 77.67.
We can see another problem in the residual plot and normal Q-Q plot, but let me stop here.


### \noindent Problem9.
**Albert and Rizzo, Chapter 7, problem 7**

**7.7 (cars data). For the cars data in Example 7.1, compare the coefficient**
**of determination $R^2$ for the two models (with and without intercept term in**
**the model). Hint: Save the fitted model as L and use summary(L) to display**
**$R^2$. Interpret the value of $R^2$ as a measure of the fit.**

```{r}
data(cars)
head(cars)
plot(cars)
```

Two models are

with intercept:
\[dist = \beta_0 + \beta_1 speed + \epsilon\]

without intercept:
\[dist = \beta_1 speed + \epsilon\]

with standard assumptions of simple linear regression models.

```{r}
car_lmfit_with_intercept <- lm(dist ~ speed, data = cars)
car_lmfit_without_intercept <- lm(dist ~ 0 + speed, data = cars)
summary(car_lmfit_with_intercept)
summary(car_lmfit_without_intercept)
```

Note that in F-test for each model and all t-test for each estimated coefficient,
we can reject all H0s. (But, I cannot be sure how to interpret the F-test's result
of the non-intercept case.)

For 'with-intercept' case, $R^2$ is 0.6511, and for 'without intercept case', $R^2$ is 0.8963.
However, I think these results are weird. 
Perhaps R may calculate the $R^2$ value wrongly for the non-intercept linear model.


```{r}
par(mfrow = c(1, 3))
plot(dist ~ speed, data = cars, xlim = c(0, 25))
abline(car_lmfit_with_intercept, col = "blue")
abline(car_lmfit_without_intercept, col = "red")
legend("topleft", col = c("blue", "red"), lty = c(1, 1),
  legend = c("with intercept", "without intercept"))

car_lmfit_with_intercept_sse <- sum((car_lmfit_with_intercept$residuals)^2)
car_lmfit_without_intercept_sse <- sum((car_lmfit_without_intercept$residuals)^2)

plot(cars$speed, car_lmfit_with_intercept$residuals, main = "residuals: with intercept")
abline(h = 0)
text(5, 43, labels = paste("SSE:", round(car_lmfit_with_intercept_sse)), cex = 1.5, pos=4)

plot(cars$speed, car_lmfit_without_intercept$residuals, main = "residuals: without intercept")
abline(h = 0)
text(5, 50, labels = paste("SSE:", round(car_lmfit_without_intercept_sse)), cex = 1.5, pos=4)
```

The reason is, the fitting result of the model with an intercept seems much better than the result of the model without intercept
The blue line clearly better than the red line in the scatterplot. 
Not only that, points in the residual plot of the model with an intercept have smaller values than without-intercept case.
If I calculate the SSE value by myself, the SSE of the with-intercept model is less than the SSE of the without-intercept model.

So, I think we need to be careful when interpreting the $R^2$ value of a linear model without an intercept term.



### \noindent Problem10.

**Albert and Rizzo, Chapter 7, problem 11**

**7.11 (twins data). Import the data file "twins.txt" using read.table. (The**
**commands to read this data file are shown in the twins example in Section**
**3.3, page 85.) The variable DLHRWAGE is the difference (twin 1 minus twin 2)**
**in the logarithm of hourly wage, given in dollars. The variable HRWAGEL is**
**the hourly wage of twin 1. Fit and analyze a simple linear regression model**
**to predict the difference DLHRWAGE given the logarithm of the hourly wage of**
**twin 1.**

First, let me load the dataset. For convenience, I will exclude all rows
with at least one NA value for any variable.
(I admit that this is an extreme(?) approach, 
but I don't want to fill in incomplete data using other statistical methods here.)

```{r}
twins_raw <- read.table("Rx-data/twins.txt",
  header = TRUE, sep = ",", na.strings = ".")
twins <- subset(twins_raw, complete.cases(twins_raw)) #exclude NA
names(twins)
head(twins)
```

Let's do EDA.

```{r}
par(mfrow = c(1, 2))
plot(DLHRWAGE ~ HRWAGEL, data = twins, main = "before log transform")
plot(DLHRWAGE ~ log(HRWAGEL), data = twins, 
  xlim = c(0, 5), main = "after log transform")
```

Even if we take the log transform, 
the relationship between 'DLHRWAGE' and 'log(HRWAGEL)' does not seem linear.
Nevertheless, I will fit a linear model, following the direction of the problem.

Here is our model.

\[DLHRWAGE = \beta_0 + \beta_1 HRWAGEL  + \epsilon\]

with standard assumptions of simple linear regression models.

```{r}
twins_lm_fit <- lm(DLHRWAGE ~ log(HRWAGEL), data = twins)
summary(twins_lm_fit)

par(mfrow = c(1, 2))
plot(DLHRWAGE ~ log(HRWAGEL), data = twins,
  xlim = c(0, 5), main = "linear model fit")
abline(twins_lm_fit)

plot(log(twins$HRWAGEL), twins_lm_fit$residuals,
  main = "residual plot", xlab = "log(HRWAGEL)")
abline(h = 0)
```

By F test's p-value, we can reject the H0 
(H0: $\beta_1 = 0$ vs H1: $\beta_1 \neq 0$). 
And, at the t-test, we can also reject H0.
(Actually, F test and t-test for $\beta_1$ are equivalent.)
And $R^2$ is 0.2509. I think this is pretty low.

Personally, I cannot satisfy this kind of result. The fitting is poor.
But if you don't matter whatever, you can use this model to predict the 'DLHRWAGE' value
given 'log(HRWAGEL)' value. If you want to use R, then just use 'predict' function.
For example,
```{r}
new_log_HRWAGEL <- seq(0, 5, by=0.5)
new_HRWAGEL_df <- data.frame(HRWAGEL = exp(new_log_HRWAGEL))
# 1.14801 + log(new_x) * -0.46055
conf_int <- predict(twins_lm_fit, new_HRWAGEL_df, interval = "confidence")
pred_int <- predict(twins_lm_fit, new_HRWAGEL_df, interval = "prediction")

par(mfrow = c(1, 2))
plot(DLHRWAGE ~ log(HRWAGEL), data = twins,
  xlim = c(0, 5), main = "confidence interval")
abline(twins_lm_fit)
lines(new_log_HRWAGEL, conf_int[, 2], col = "red")
lines(new_log_HRWAGEL, conf_int[, 3], col = "red")

plot(DLHRWAGE ~ log(HRWAGEL), data = twins,
  xlim = c(0, 5), main = "prediction interval")
abline(twins_lm_fit)
lines(new_log_HRWAGEL, pred_int[, 2], col = "blue")
lines(new_log_HRWAGEL, pred_int[, 3], col = "blue")
```



### \noindent Problem11.

**James et al., Chapter 2, problem 10**

**This exercise involves the Boston housing data set.**

**(a) To begin, load in the Boston data set. The Boston data set is**
**part of the MASS library in R.**
```{r}
library(MASS)
```
**Now the data set is contained in the object Boston.**
```{r}
head(Boston)
names(Boston)
```

**Read about the data set:**
```{r}
# ?Boston
```
**How many rows are in this data set? How many columns? What**
**do the rows and columns represent?**

According to the Boston dataset explanation,
there is 506 rows and 14 columns.
Each rows represent each town in Boston suburbs,
and each columes represent


- crim: per capita crime rate by town.
- zn: proportion of residential land zoned for lots over 25,000 sq.ft.
- indus: proportion of non-retail business acres per town.
- chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
- nox: nitrogen oxides concentration (parts per 10 million).
- rm: average number of rooms per dwelling.
- age: proportion of owner-occupied units built prior to 1940.
- dis: weighted mean of distances to five Boston employment centres.
- rad: index of accessibility to radial highways.
- tax: full-value property-tax rate per \$10,000.
- ptratio: pupil-teacher ratio by town.
- black: $1000(Bk - 0.63)^2$ where Bk is the proportion of blacks by town.
- lstat: lower status of the population (percent).
- medv: median value of owner-occupied homes in \$1000s.


**(b) Make some pairwise scatterplots of the predictors (columns) in**
**this data set. Describe your findings.**

```{r, fig.height=7.5, echo=FALSE}
panel.hist <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor)
}

pairs(Boston[,c(-4)],  upper.panel = panel.cor, panel = panel.smooth,
      cex = 1, pch = 20, bg = "light blue", horOdd=TRUE,
      diag.panel = panel.hist, cex.labels = 2, font.labels = 1)
```

Oh, this problem is the same as in the last homework, so I repeat the answer to that.

First, 'zn', 'rad', 'tax' and 'black' have uniquely shaped histograms. 
In the case of the 'zn', most of the points are near 0.
In contrast, most of the points of the 'black' have large values.
Finally, the 'rad' and 'tax' seem to have two clusters.

Next, 'indus'and 'nox', 'indus' and 'age', 'indus' and 'dis', 'nox' and 'age', 'nox' and 'dis'
'rm' and 'lstat', 'rm' and 'medv', 'lstat' and 'medv' have some increasing/decreasing relationships.
However, all cases do not seem perfectly linear. Perhaps, we may get the linear relationship among them by some transformations.

Finally, some fitted resistant lines and correlation coefficients (especially  in the 'rad' and 'tax' case) do not give much useful information. 
As you can see in these examples, it always seems necessary to check visually through pictures for a EDA procedure.



**(c) Are any of the predictors associated with per capita crime rate?**
**If so, explain the relationship.**
```{r, echo=FALSE, fig.height=2}
names(Boston)
par(mfrow = c(1, 3))
plot(crim ~ zn, data = Boston)
plot(crim ~ indus, data = Boston)
plot(crim ~ chas, data = Boston)

plot(crim ~ nox, data = Boston)
plot(crim ~ rm, data = Boston)
plot(crim ~ age, data = Boston)

plot(crim ~ dis, data = Boston)
plot(crim ~ rad, data = Boston)
plot(crim ~ tax, data = Boston)

plot(crim ~ ptratio, data = Boston)
plot(crim ~ black, data = Boston)
plot(crim ~ lstat, data = Boston)

plot(crim ~ medv, data = Boston)
```

Although none of the predictors are very satisfactory to be associated with per capita crime rate,
we can say that there is no high crime-rate town unless 'zn' is 0 and 'indus' is between 15 and 20.
Likewise, 'rad', 'tax', 'ptratio' can be used similar way. (the high crame rate towns have 'rad' values greater than 20,
'tax' value grater than 600, and 'ptratio' near 20.)
'lstat' and  'medv' seem to have weak quadratic or exponential-like relationships. But I think they are too weak.


**(d) Do any of the suburbs of Boston appear to have particularly**
**high crime rates? Tax rates? Pupil-teacher ratios? Comment on**
**the range of each predictor.**

```{r}
range(Boston$crim)
range(Boston$tax)
range(Boston$ptratio)

par(mfrow=c(1,3))
boxplot(Boston["crim"], xlab = "crim rates")
boxplot(Boston["tax"], xlab = "tax rates")
boxplot(Boston["ptratio"], xlab = "pupil-teacher ratio")
```

In the case of crime rates, there are lots of suburbs that have extremely high crime rates compared to the other towns.
However, although the range of tax rates is large, they don't seem particularly high, according to the boxplot.
Likewise, the difference between the maximum and the minimum pupil-teacher rates is quite big (Yes, they are 'ratios'!),
but we cannot say that the maximum value is particularly high.


**(e) How many of the suburbs in this data set bound the Charles river?**
```{r}
table(factor(Boston$chas))
```

35 suburbs set bound the Charles river.


**(f) What is the median pupil-teacher ratio among the towns in this data set?**
```{r}
median(Boston$ptratio)
```

The median pupil-teacher ratio is 19.05.


**(g) Which suburb of Boston has lowest median value of owner occupied homes? What are the values of the other predictors**
**for that suburb, and how do those values compare to the overall**
**ranges for those predictors? Comment on your findings.**

```{r}
which.min(Boston$medv)
Boston[which.min(Boston$medv),]
```

The suburb of the 399th row has the lowest median value of owner-occupied homes.

```{r, fig.height=2}
par(mfrow = c(1, 3))
for(i in 1:12){
  boxplot(Boston[i], xlab = names(Boston[i]), horizontal = TRUE)
  points(Boston[399, i], 1, col = "red", pch = 16, cex = 3)
}
```

The red dot corresponds to the 399th suburb.
From the boxplots, we can find that the 399th suburb also has extreme values 
for 'zn', 'age', 'dis', 'rad', 'black' and quite high value of 'crim', 'nox', 'tax'
compared to the overall range of each predictor.


**(h) In this data set, how many of the suburbs average more than**
**seven rooms per dwelling? More than eight rooms per dwelling?**
**Comment on the suburbs that average more than eight rooms**
**per dwelling.**

```{r}
sum(Boston$rm > 7)
sum(Boston$rm > 8)
```

64 suburbs have more than seven rooms per dwelling on average.
And, only 13 suburbs have more than eight rooms per dwelling on average.

```{r, fig.height=2}
par(mfrow = c(1, 3))
for(i in 1:12) {
  boxplot(Boston[i], xlab = names(Boston[i]), horizontal = TRUE)
  eightroom_idx <- (Boston$rm > 8)
  points(Boston[eightroom_idx, i], rnorm(sum(eightroom_idx), 1, 0.1),
    col = "blue", pch = 16, cex = 1.5)
}
```

Blue dots correspond to suburbs that have more than eight rooms per dwelling on average.
Their per capita crime rates are low, and the proportions of blacks are high.
Moreover, most of them have low 'zn', 'indus', 'dis', 'rad', and 'tax' values and high 'age' values.



### \noindent Problem12.

**James et al., Chapter 3, problem 1**

**Describe the null hypotheses to which the p-values given in Table 3.4**
**correspond. Explain what conclusions you can draw based on these**
**p-values. Your explanation should be phrased in terms of sales, TV,**
**radio, and newspaper, rather than in terms of the coefficients of the**
**linear model.**

```{r}
table3.4 <- data.frame(
  variable = c("Intercept", "TV", "radio", "newspaper"),
  coefficient = c(2.939, 0.046, 0.189, -0.001),
  Std.Error = c(0.3119, 0.0014, 0.0086, 0.0059),
  t_statistic = c(9.42, 32.81, 21.89, -0.18),
  p_value = c("<0.0001", "<0.0001", "<0.0001", "0.8599")
)
library(knitr)
kable(table3.4)
```

This is t-test for a multiple linear regression with model
\[sales = \beta_0 + \beta_1 TV + \beta_2 radio + \beta_3 newspaper + \epsilon\]

We can test four kinds of hypotheses.

- For TV

H0 : TV advertising has no effect on sales. ($\beta_1 = 0$)

H1 : TV advertising affects sales. ($\beta_1 \neq 0$)

The p-value is <0.001, so we can reject H0 with common significance levels like 0.05 or 0.01.
Thus, TV advertising affects sales.


- For radio

H0 : Radio advertising has no effect on sales. ($\beta_2 = 0$)

H1 : Radio advertising affects sales. ($\beta_2 \neq 0$)

The p-value is <0.001, so we can reject H0 with common significance levels like 0.05 or 0.01.
Thus, Radio advertising affects sales.


- For radio

H0 : Newspaper advertising has no effect on sales. ($\beta_3 = 0$)

H1 : Newspaper advertising affects sales. ($\beta_3 \neq 0$)

The p-value is 0.8599, so we cannot reject H0 with common significance levels like 0.05 or 0.01.
Thus, we cannot conclude that newspaper advertising affects sales.


And, finally,

- For intercept

H0 : There are no sales without any advertising. ($\beta_0 = 0$)

H1 : There is a non-zero, basic level of sales. ($\beta_0 \neq 0$)

The p-value is <0.001, so we can reject H0 with common significance levels like 0.05 or 0.01.
Thus, although there is no advertisement on TV, radio, or newspaper, the level of sales is not 0.



### \noindent Problem13.

**James et al., Chapter 3, problem 3**

**Suppose we have a data set with five predictors, X1 = GPA, X2 = IQ,**
**X3 = Gender (1 for Female and 0 for Male), X4 = Interaction between**
**GPA and IQ, and X5 = Interaction between GPA and Gender. The**
**response is starting salary after graduation (in thousands of dollars).**
**Suppose we use least squares to fit the model, and get**
$\hat{\beta_0}=50, \hat{\beta_1}=20, \hat{\beta_2}=0.07, \hat{\beta_3}=35,
\hat{\beta_4}=0.01, \hat{\beta_5}=-10$.

Before solving the problem, let me write down the fitted model.

\[StartingSalary = 50 + 20 * GPA + 0.07 * IQ + 35 * Gender + 0.01 * GPA * IQ  - 10 *  GPA * Gender\]

**(a) Which answer is correct, and why?**

1. For a fixed value of IQ and GPA, males earn more on average than females.

False. 
\[MaleSalary = 50 + 20 * GPA + 0.07 * IQ + 0.01 * GPA * IQ\]
\[FemaleSalary = (50+35) + (20-10) * GPA + 0.07 * IQ + 0.01 * GPA * IQ\]

If given GPA satisfies $35 > 10*GPA$, females earn more on average than males.


2. For a fixed value of IQ and GPA, females earn more on average than males.

False.
If given GPA satisfies $35 < 10*GPA$, males earn more on average than females.

3. For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough.

True. (same reason as 2)

4. For a fixed value of IQ and GPA, females earn more on average than males provided that the GPA is high enough.

False. (same reason as 1)


**(b) Predict the salary of a female with IQ of 110 and a GPA of 4.0.**

\[femaleSalary|_{IQ=110, GPA=4.0} = (50+35) + (20-10) * 4.0 + 0.07 * 110 + 0.01 * 4.0 * 110 = 137.1\]

The predicted salary is 137.1 thousands of dollars.


**(c) True or false: Since the coefficient for the GPA/IQ interaction**
**term is very small, there is very little evidence of an interaction**
**effect. Justify your answer**

False in general, although it is likely. (more precisely, we could not know under given information.)
To check the significance of the interaction term, we should consider the standard error of the model,
and conduct the t-test for the term. But information about the standard error is missing in this problem.


### \noindent Problem14.

**James et al., Chapter 3, problem 8**

**This question involves the use of simple linear regression on the Auto data set.**

**(a) Use the lm() function to perform a simple linear regression with**
**mpg as the response and horsepower as the predictor. Use the**
**summary() function to print the results. Comment on the output.**
**For example:**

**1. Is there a relationship between the predictor and the response?**

**2. How strong is the relationship between the predictor and the response?**

**3. Is the relationship between the predictor and the response positive or negative?**

**4. What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals?**

```{r}
auto <- read.csv("ItSL-data/Auto.csv", na.strings = "?")
head(auto)
```

```{r}
par(mfrow = c(1, 1))
plot(mpg ~ horsepower, data = auto)
```

There is a negative relation, but it seems quadratic or exponential, not linear.

```{r}
auto_smpl_lm_fit <- lm(mpg ~ as.numeric(horsepower), data = auto)
summary(auto_smpl_lm_fit)
```

When we use a linear model to fit the relationship between 'mpg' and 'horsepower',
the estimated slope is -0.157845. In other words,
\[mpg = 39.9358 - 0.157845 * horsepower\]

$R^2$ is pretty good, although it is not very high.

Let's take a look at the t-test result. (In this case, t-test and F-test are the same.)
Hypotheses are

H0: $\beta_1 = 0$

H1: $\beta_1 \neq 0$

The coefficient's t-statistic value is -24.49, 
following student-t distribution with 390 df under H0.
The p-value is very small (<2e-16), so we can reject H0 with common significance levels like 0.05 or 0.01.
So the relationship is statistically significant.
To sum up, the relationship is negative, and -0.157845 is not 0 statistically.

Finally, to get the confidence and prediction interval,

```{r}
new_horsepower_df <- data.frame(horsepower = c(98))
mpg_conf_int <- predict(auto_smpl_lm_fit, new_horsepower_df,
  interval = "confidence", level = 0.95)
mpg_pred_int <- predict(auto_smpl_lm_fit, new_horsepower_df,
  interval = "prediction", level = 0.95)
mpg_conf_int
mpg_pred_int
```

The 95% confidence interval at horsepower of 98 is $(23.97308, 24.96108)$.
And, the 95% prediction interval at horsepower of 98 is $(14.8094, 34.12476)$.
Because the prediction interval considers additional error,
it is reasonable to be wider than the confidence interval.


**(b) Plot the response and the predictor. Use the abline() function**
**to display the least squares regression line.**

```{r}
plot(mpg ~ horsepower, data = auto, main = "auto")
abline(auto_smpl_lm_fit)
```


**(c) Use the plot() function to produce diagnostic plots of the least**
**squares regression fit. Comment on any problems you see with the fit.**

```{r}
par(mfrow = c(2, 2))
plot(auto_smpl_lm_fit)
```

Diagnostic plots show that the fitting is bad.
Residual plot has an U-shape pattern, and variance of residual increases as fitted value increase.
It seems that the data does not follow the normally assumption. Points on the normal Q-Q plot are not on the one straight line.
Not only that, there are so many points having high leverage values.

I think that the reason that the linear model fit is poor is that,
as I pointed out above, two variables' relation seems quadratic or exponential shape.
We should have applied transform or have used other non-linear models.


### \noindent Problem15.

**James et al., Chapter 3, problem 9**

**This question involves the use of multiple linear regression on the**
**Auto data set.**
```{r}
auto <- read.csv("ItSL-data/Auto.csv", na.strings = "?")
summary(auto)
auto$horsepower <- as.numeric(auto$horsepower)
auto$origin <- as.factor(auto$origin)
auto <- subset(auto, complete.cases(auto))

```

**(a) Produce a scatterplot matrix which includes all of the variables**
**in the data set.**

I include numeric variables in the scatterplot matrix.
```{r}
pairs(auto[, -c(8, 9)])
names(auto)
```

The 'mpg' seems to have negative, non-linear relations with 'displacement',
'horsepower', and 'weight' variables. 
Additionally, The 'mpg' seems negative linear relation with the 'cylinders'.
There are linear relationship among 'cylinders', 'displacement', 'horsepower', and 'weight' variables.
The 'acceleration' has negative relationship with 'cylinders', 'displacement', 'horsepower', and 'weight' variables.


**(b) Compute the matrix of correlations between the variables using**
**the function cor(). You will need to exclude the name variable,**
**cor() which is qualitative.**

```{r}
cor(auto[, -c(8, 9)])
```

From this correlation matrix, we can get some information about 
how strong the linear relation is of each pair.
For a linear relationship, interpretation of correlations is the same as the answer in (a) can be obtained.



**(c) Use the lm() function to perform a multiple linear regression**
**with mpg as the response and all other variables except name as**
**the predictors. Use the summary() function to print the results.**
**Comment on the output. For instance:**

```{r}
auto_multi_lm_fit <- lm(mpg ~ cylinders + displacement + horsepower + weight +
  acceleration + year + origin, data = auto)
summary(auto_multi_lm_fit)
```

**1. Is there a relationship between the predictors and the response?**

Yes. To answer this question, we can do F-test.
Let $\beta_i, i=1,...,8$ be coefficients in the model (except the intercept.)

H0: $\beta_1 = \beta_2 = ... = \beta_8 = 0$

H1: At least one $\beta_i, i=1, ..., 8$ is not 0.

The F-statistic is 224.5, which follows the F distribution with df 8 and 383.
The p-value is near 0, so we can reject H0 under a common significance level like 0.05 or 0.01.
Thus, this relationship is significant. We can say that there is a relationship between the predictors and the response.


**2. Which predictors appear to have a statistically significant relationship to the response?**

For this question, we can conduct a t-test for each predictor.

For $i=1,2,...,8$, we can set 8 pair of hypotheses,

H0: $\beta_i = 0$

H1: $\beta_i \neq 0$

Each t-statistic follows the student-t distribution with df=383 under H0.

According to the R output,
'displacement', 'weight', 'year', and 'origin' are significant under 0.05 significance level.
So, we can say that the four variables have statistically significant relationships to the 'mpg'. 


**3. What does the coefficient for the year variable suggest?**

The coefficient is 0.777. In other words, if other variables are fixed,
mpg will increase by 0.777 as the year increases 1.


**(d) Use the plot() function to produce diagnostic plots of the linear**
**regression fit. Comment on any problems you see with the fit.**
**Do the residual plots suggest any unusually large outliers? Does**
**the leverage plot identify any observations with unusually high**
**leverage?**

```{r}
par(mfrow = c(2, 2))
plot(auto_multi_lm_fit)
```

Like the simple-linear regression case, diagnostic plots show that the fitting result is poor again.
The residual plot has a U-shape pattern, and the variance of residual increases as the fitted value increases.
We may consider some points with absolute residual values ​​greater than 10 as outliers,
although the points are quite close to other residual values.

Moreover, It seems that the data does not follow the normal distribution assumption.
Points on the normal Q-Q plot are not on the one straight line at the right tail.
Not only that, there is a point (14th) having high leverage value.



**(e) Use the * and : symbols to fit linear regression models with**
**interaction effects. Do any interactions appear to be statistically**
**significant?**

Since adding all the interaction terms makes the model too complex, I've only added a few.

```{r}
auto_multi_lm_fit_interaction <- lm(mpg ~ cylinders + displacement + horsepower + weight +
  acceleration * origin + year * origin, data = auto)
summary(auto_multi_lm_fit_interaction)
```

Yes, interaction terms within year and origin, and within acceleration and origin are
statistically significant (under the significance level 0.05.)


**(f) Try a few different transformations of the variables, such as**
$log(X), \sqrt{X}, X^2$.
**Comment on your findings.**

First, let's take the logarithm of 'mpg'.
```{r}
auto_multi_lm_fit_logY <- lm(log(mpg) ~ cylinders + displacement + horsepower + weight +
  acceleration + year + origin, data = auto)
summary(auto_multi_lm_fit_logY)
par(mfrow = c(2, 2))
plot(auto_multi_lm_fit_logY)
```

The $R^2$ is slightly improved.
The residual plot becomes much better, but the normal Q-Q plot is bad at the left tail.
Not only that, there is still a point that has too high leverage values.

Next is 'sqrt(mpg)'.

```{r}
auto_multi_lm_fit_sqrtY <- lm(sqrt(mpg) ~ cylinders + displacement + horsepower + weight +
  acceleration + year + origin, data = auto)
summary(auto_multi_lm_fit_sqrtY)
par(mfrow = c(2, 2))
plot(auto_multi_lm_fit_sqrtY)
```

The residual plot of sqrt(mpg) is better than the non-transform case, 
but worse than the log(mpg) case.
Other issues still remain.

Finally, let's add some square terms for some predictors.

```{r}
auto_multi_lm_fit_Xsquare <- lm(mpg ~ cylinders + displacement + I(displacement)^2 +
  horsepower + I(horsepower)^2 +  weight + I(weight)^2 +
  acceleration + year + origin, data = auto)
summary(auto_multi_lm_fit_Xsquare)
par(mfrow = c(2, 2))
plot(auto_multi_lm_fit_Xsquare)
```

Although adding square terms for some predictors, 
it does not work well.



### \noindent Problem16.

**James et al., Chapter 3, problem 10**

**This question should be answered using the Carseats data set.**

```{r}
carseats <- read.csv("ItSL-data/Carseats.csv")
names(carseats)
head(carseats)
summary(carseats)
carseats$ShelveLoc <- factor(carseats$ShelveLoc)
carseats$Urban <- factor(carseats$Urban)
carseats$US <- factor(carseats$US)
```

**(a) Fit a multiple regression model to predict Sales using Price,**
**Urban, and US.**

Let's do EDA.

```{r}
par(mfrow = c(1, 3))
plot(Sales ~ Price, data = carseats)
plot(Sales ~ Urban, data = carseats)
plot(Sales ~ US, data = carseats)

par(mfrow = c(1, 1))
plot(Sales ~ Price, data = carseats,
  col = Urban, pch = as.numeric(carseats$US))
legend("topright", col=c(1,2,1,2), pch=c(1,1,2,2),
  legend = c("Not Urban-Not US", "Urban-Not US", "Not Urban- US", "Urban-US"))
```

Because Urban and US are factors, we can draw a figure.
Points are too spread out to be considered a linear relationship.
In all cases, we can observe that there are weak negative relationships.

Anyway, let's fit the data using the given linear model.
Since there is no mention of the interaction term in the problem, I omit the term.

```{r}
carseats_lm_fit <- lm(Sales ~ Price + Urban + US, data = carseats)
summary(carseats_lm_fit)
```

```{r, echo=FALSE}
par(mfrow = c(1, 1))
plot(Sales ~ Price, data = carseats,
  col = Urban, pch = as.numeric(carseats$US))
legend("topright", col=c(1,2,1,2), pch=c(1,1,2,2),
  legend = c("Not Urban-Not US", "Urban-Not US", "Not Urban- US", "Urban-US"))
legend("topleft", col=c(1,2,1,2), lty=c(1,1,3,3),
  legend = c("Not Urban-Not US", "Urban-Not US", "Not Urban- US", "Urban-US"))

carseats_coeff <- carseats_lm_fit$coefficients
abline(carseats_coeff[1], carseats_coeff[2])
abline(carseats_coeff[1] + carseats_coeff[3],
  carseats_coeff[2], col=2)
abline(carseats_coeff[1] + carseats_coeff[4],
  carseats_coeff[2], lty=3)
abline(carseats_coeff[1] + carseats_coeff[3] + carseats_coeff[4],
  carseats_coeff[2], col=2, lty=3)
```

It is difficult to see the difference in results depending on whether it is Urban or not,
because the coefficient of 'Urban' is close to 0.


**(b) Provide an interpretation of each coefficient in the model. Be**
**careful—some of the variables in the model are qualitative!**

```{r}
carseats_lm_fit$coefficients
```
If other variables are fixed, the value of 'Sales' decreases by 0.054459 as the 'Price' increases by 1.
If other variables are fixed, the value of 'Sales' is lower by 0.021916 when it is Urban, than when it is not Urban.
Likewise, If other variables are fixed, the value of 'Sales' is higher by 1.200573 in the US than not in the US.


**(c) Write out the model in equation form, being careful to handle**
**the qualitative variables properly.**

\[Sales = 13.0435 - 0.0219 * I(Urban) + 1.2006 * I(US) - 0.054459 * Price\]

where $I(Urban)=1$ for Urban, $I(Urban)=0$ otherwise, $I(US)=1$ for US, $I(US)=0$ otherwise.


**(d) For which of the predictors can you reject the null hypothesis**
**H0 : $\beta_j=0$ ?**

The alternative of each t-test is H1: $\beta_j \neq 0$.
Under common significance levels like 0.05 or 0.01,
I can reject the null hypothesis for coefficients of 'Price' and 'US'.
However, I cannot reject the H0 for Urban's coefficient, because its p-value is 0.936.
Statistically, the coefficient of 'Urban' is not different from 0.


**(e) On the basis of your response to the previous question, fit a**
**smaller model that only uses the predictors for which there is**
**evidence of association with the outcome.**

```{r}
carseats_lm_fit_smaller <- lm(Sales ~ Price + US, data = carseats)
summary(carseats_lm_fit_smaller)
```

Now, we can reject all t-tests' H0s because all p-values of coefficients are very close to 0.

```{r, echo=FALSE}
par(mfrow = c(1, 1))
plot(Sales ~ Price, data = carseats, pch = as.numeric(carseats$US))
legend("topright", pch = c(1, 2),
  legend = c("Not US", "US"))
legend("topleft", lty = c(1, 3),
  legend = c("Not US", "US"))

carseats_smaller_coeff <- carseats_lm_fit_smaller$coefficients
abline(carseats_smaller_coeff[1], carseats_smaller_coeff[2])
abline(carseats_smaller_coeff[1] + carseats_smaller_coeff[3],
  carseats_smaller_coeff[2], lty = 3)
```


**(f) How well do the models in (a) and (e) fit the data?**

The $R^2$s of the models (a) and (e) are both 0.2393, which are too low.
The reason may be the data are not strongly linear, as we saw in the EDA process.

```{r, fig.height=2.5}
par(mfrow = c(1, 4))
plot(carseats_lm_fit)
plot(carseats_lm_fit_smaller)
```

Diagnostic plots are generally good, although the data tends to be clustered in the middle.


**(g) Using the model from (e), obtain 95 % confidence intervals for**
**the coefficient(s).**

I can calculate the intervals by myself using fitted value, standard error, and 'pt' function in R,
but R offers the super-convenient 'confint' function, which gives us the intervals automatically.

```{r}
round(confint(carseats_lm_fit_smaller, level=0.95),4)
```

For the coefficient of the 'price', the 95% CI is (-0.0648, -0.0442).
And, for the coefficient of the 'US', the 95% CI is (0.6915, 1.7078).

**(h) Is there evidence of outliers or high leverage observations in the**
**model from (e)?**

Although there are points having big residual values, 
I think it seems difficult to treat them as outliers or high-leverage observations.
I think the fundamental problem is that the data isn't linear overall but all spread out,
not the existence of outliers or high-leverage points.

