---
title: "Homework Assignment 5"
author: "Seokjun Choi"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

### \noindent Problem1.

**Fit a logistic regression model to the graft.vs.host data from the ISwR package**
**predicting gvhd response.**
**Use different transformations of the index variable.**

```{r}
library(ISwR)
gh_data <- graft.vs.host
# ?graft.vs.host

gh_data$type <- factor(gh_data$type)
gh_data$preg <- factor(gh_data$preg)
gh_data$gvhd <- factor(gh_data$gvhd)
gh_data$dead <- factor(gh_data$dead)

head(gh_data)
```

According to the R help file of 'graft.vs.host' data,
these data are about "Graft versus host disease" and its predictors.
The dataset consists of 37 subjects, 9 variables (1 response, 1 subject index ('pnr'), 7 predictors).
Each column means

- pnr: a numeric vector patient number.
- rcpage: a numeric vector, age of recipient (years).
- donage: a numeric vector, age of the donor (years).
- type: a numeric vector, type of leukaemia coded 1: AML, 2: ALL, 3: CML for acute myeloid, acute lymphatic, and chronic myeloid leukaemia.
- preg: a numeric vector code indicating whether donor has been pregnant. 0: no, 1: yes.
- index: a numeric vector giving an index of mixed epidermal cell-lymphocyte reactions.
- gvhd: a numeric vector code, graft-versus-host disease, 0: no, 1: yes.
- time: a numeric vector, follow-up time
- dead: a numeric vector code, 0: no (censored), 1: yes

Let's do EDA.

```{r}
names(gh_data)

## put (absolute) correlations on the upper panels,
## with size proportional to the correlations.
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...){
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

pairs(gh_data[, -1], lower.panel = panel.smooth, upper.panel = panel.cor,
      gap=0, row1attop=FALSE) #exclude 'pnr'
```

Two pairs of variables have linear relations. (between 'dead' and 'time', and between 'donage' and 'rcpage'.)

Additionally, Let's look at relationships between 'gvhd' and other variables in detail.

```{r}
par(mfrow = c(1, 3))
plot(jitter(as.numeric(gvhd), amount = 0.1) ~ rcpage,
  data = gh_data, pch = 19, ylab = "gvhd")
plot(jitter(as.numeric(gvhd), amount = 0.1) ~ donage,
  data = gh_data, pch = 19, ylab = "gvhd")
plot(jitter(as.numeric(gvhd), amount = 0.1) ~ time,
  data = gh_data, pch = 19, ylab = "gvhd")
plot(gvhd ~ type, data = gh_data)
plot(gvhd ~ preg, data = gh_data)
plot(gvhd ~ dead, data = gh_data)
```

If values of two continuous variables 'rcpage' and 'donage' are higher,
it seems that the possibility of having graft-versus-host disease tends to increase.
In the case of another continuous variable 'time', the chance of g-v-h disease seems less
when the time increase.

Next, let's look at the 'index' variable, which is mentioned specifically in the problem.
To choose a good transformation for 'index' variable,
I draw four figures with
log-transform ($\sim x^0$), square root transform($\sim x^{1/2}$),
data without transform($x^1$), and square transform($x^2$), respectively.

```{r}
par(mfrow = c(2, 2))
plot(jitter(as.numeric(gvhd), amount = 0.1) ~ log(index),
  data = gh_data, pch = 19, ylab = "gvhd")
plot(jitter(as.numeric(gvhd), amount = 0.1) ~ sqrt(index),
  data = gh_data, pch = 19, ylab = "gvhd")
plot(jitter(as.numeric(gvhd), amount = 0.1) ~ index,
  data = gh_data, pch = 19, ylab = "gvhd")
plot(jitter(as.numeric(gvhd), amount = 0.1) ~ (index)^2,
  data = gh_data, pch = 19, ylab = "gvhd")
```

Among four cases, the log-transform case seems the best.
This is because the 'gvhd' seems to be differentiated better
by the log-'index' value than in other cases.
Thus, I choose the log transform as the transform of 'index' variable.

I will exclude all interaction terms.
This is simply because I want to avoid complex models.
(I may try to justify this decision more by looking at the EDA step
by drawing all pairs of marginal-joint plots with 'gvhd' and other two predictors.
Or, I may deviance test for model comparison.
However, I will omit these things here because they will be too long.)

Then, our first model is

\[logit(P(y_i = 1 | x_{i1}, x_{i2}, x_{i3}, x_{i4}, x_{i5}, x_{i6}, x_{i7}, x_{i8})) = 
  \beta_0 + \alpha_1 x_{i1} + \alpha_2 x_{i2} + \alpha_3 x_{i3} + \alpha_4 x_{i4}
  + \beta_5 x_{i5} + \beta_6 x_{i6} + \beta_7 x_{i7} + \beta_8 log{x_{i8}} \]
for i = 1,2,...,37, where

- $y_i$: (binary factor) 1 if i-th patient has graft-versus-host disease, 0 otherwise
- $x_{i1}$: (binary factor) 1 if i-th donor has been pregnant. 0 otherwise
- $x_{i2}$: (dummy variable for the 3-level factor, 'type') i-th patient's leukaemia type of i-th sample. 0 if AML or CML, 1 if ALL.
- $x_{i3}$: (dummy variable for the 3-level factor, 'type') i-th patient's leukaemia type of i-th sample. 0 if AML or ALL, 1 CML.
- $x_{i4}$: (binary factor) 1 if i-th patient is dead. 0 otherwise (censored)
- $x_{i5}$: i-th data point's age of recipient (years)
- $x_{i6}$: i-th data point's age of donor (years).
- $x_{i7}$: i-th data point's follow up time.
- $x_{i8}$: i-th data point's index of mixed epidermal cell-lymphocyte reactions

Note that, I used dummy-variable notation since there are too many factor variables.
Also note that, because 'type' factor has three levels, there are two terms for 'type' variable in this model.
($x_{i2} and x_{i3}$)

```{r}
gh_glm_fit_full <- glm(
  gvhd ~ type + preg + dead + rcpage + donage + time + log(index),
  data = gh_data, family = binomial)
summary(gh_glm_fit_full)
```

Unfortunately, the numerical algorithm to fit this model failed.
I cannot be sure the exact reason, but one possible guess is the colinearity between the predictors.

Let's go to back to the 'pairs' plot in EDA procedure. 
There is a pair ('time' and 'type') with correlation 0.75.
Let's take a closer look at the relationship between the response variable 'gvhd' and each variable,
and between these two variable.

```{r}
par(mfrow = c(1, 3))
plot(gvhd ~ time, data = gh_data)
plot(gvhd ~ type, data = gh_data)
plot(time ~ type, data = gh_data)
```

In the third plot, we can again confirm that there is a strong positive relationship between two predictors.
However, the patterns in mosaic plots between each predictor and the 'gvhd' are different.
It is a kind of inconsistency, so it may cause a convergence problem in the numerical method.
So, in here, I'll delete two terms related to the 'type' variable ($\alpha_2 x_{i2} and \alpha_3 x_{i3}$).

Then our model becomes

\[logit(P(y_i = 1 | x_{i1}, x_{i4}, x_{i5}, x_{i6}, x_{i7}, x_{i8})) = 
  \beta_0 + \alpha_1 x_{i1} +  \alpha_4 x_{i4}
  + \beta_5 x_{i5} + \beta_6 x_{i6} + \beta_7 x_{i7} + \beta_8 log{x_{i8}} \]
for i = 1,2,...,37, with the same notation as original model,


```{r}
gh_glm_fit_wo_type <- glm(
  gvhd ~ preg + dead + rcpage + donage + time + log(index),
  data = gh_data, family = binomial)
summary(gh_glm_fit_wo_type)
```

Fitting is successful (even though 8 iterations were run).
However, there are still so many insignificant variables in our model
when we think about their t-statistics and p-values.
But it is also known that a wald-type test is poor in the context of logistic regression.
Therefore, after I try to delete some variables,
I will decide whether I include the variable or not in our model using AIC criterion.


Here is a result-sequence using a kind of backward selection.
```{r}
# after delete preg
gh_glm_fit_3 <- glm(gvhd ~ dead + rcpage + donage + time + log(index),
  data = gh_data, family = binomial)
summary(gh_glm_fit_3)

# after delete rcpage
gh_glm_fit_4 <- glm(gvhd ~ dead + donage + time + log(index),
  data = gh_data, family = binomial)
summary(gh_glm_fit_4)

# after delete dead
gh_glm_fit_5 <- glm(gvhd ~ donage + time + log(index),
  data = gh_data, family = binomial)
summary(gh_glm_fit_5)

# after delete donage
gh_glm_fit_6 <- glm(gvhd ~ time + log(index),
  data = gh_data, family = binomial)
summary(gh_glm_fit_6)
```

The lowest AIC was achieved when we fit the 'gh_glm_fit_4'.
So, I will use the model as our final model.

Then, now our model is

\[logit(P(y_i = 1 | x_{i4}, x_{i6}, x_{i7}, x_{i8})) = 
  \beta_0 + \alpha_4 x_{i4}
  + \beta_6 x_{i6} + \beta_7 x_{i7} + \beta_8 log{x_{i8}} \]
for i = 1,2,...,37, with 

- $y_i$: 1 if i-th patient has graft-versus-host disease, 0 otherwise
- $x_{i4}$: 1 if i-th patient is dead. 0 otherwise (censored)
- $x_{i6}$: i-th data point's age of donor (years).
- $x_{i7}$: i-th data point's follow up time.
- $x_{i8}$: i-th data point's index of mixed epidermal cell-lymphocyte reactions

and $\alpha_4, \beta_6, \beta_7, \beta_8$ are effects of these variables, respectively.
The fitting result is,

```{r}
#same as 'gh_glm_fit_4'
gh_glm_fit_final <- glm(gvhd ~ dead + donage + time + log(index),
  data = gh_data, family = binomial)
summary(gh_glm_fit_final)
```

Before interpreting fitted coefficients, let's check whether this model is significant or not.

H0 : $\alpha_4 = \beta_6 = \beta_7 = \beta_8$ = 0$

H1 : at least one parameter in H0 is not 0.

Under the H0, the difference between the null-deviance and the residual deviance
follows the chi-square distribution with degrees of freedom $36-32 = 4$.

```{r}
(gh_glm_chi_statistic <- 51.049 - 15.945)
(gh_glm_chi_p_value <- 1 - pchisq(51.049 - 15.945, 36 - 32))
```

The test statistic value is 35.104, and the p-value of this test is very close to 0.
So under common significance levels like 0.05 or 0.01, we can reject H0.
Thus, this model is statistically significant.

Next, let's look at each coefficient.
When the i-th patient is not dead,
\[\frac{P(y_i = 1 | x_{i4}=0, x_{i6}, x_{i7}, x_{i8})}
    {P(y_i = 0 | x_{i4}=0, x_{i6}, x_{i7}, x_{i8})} = 
  exp(-0.68 + 0.21 x_{i6} - 0.01 x_{i7} + 5.08 log{x_{i8}})\]

Otherwise, if the i-th patient is dead,
\[\frac{P(y_i = 1 | x_{i4}=1, x_{i6}, x_{i7}, x_{i8})}
    {P(y_i = 0 | x_{i4}=1, x_{i6}, x_{i7}, x_{i8})} = 
  exp(-0.68 -3.70 + 0.21 x_{i6} - 0.01 x_{i7} + 5.08 log{x_{i8}})\]

Each coefficient can be interpreted as the (multiplicative) effect to the odds by the one-unit increased predictor.
For example, if the log-transformed index increase 1 unit (or, in other words, if the index value get multiplied by $e$,)
and if other variables stay fixed value at the same time,
the odds would be multiplied by $e^{5.08}$.
Or equivalently, the odds ratio is
\[\frac{P(Y=1|log{x_8}=c+1)/P(Y=0|log{x_8}=c+1)}{P(Y=1|log{x_8}=c)/P(Y=0|log{x_8}=c)} = e^{5.08}\]
for any $c\in \mathbb{R}$ and under other variables are fixed.


Let's evaluate this fitting result.
A better model evaluation would have been possible if I divided the training and test data from the beginning. 
But I will omit it here since we started without dividing our dataset.
So I note that there is a limitation in the analysis below, 
because the evaluation using prediction is only the evaluation of the training error. 

```{r}
gh_predict_dataframe <- gh_data[, c("dead", "donage", "time", "index")]
gh_pred_prob <- predict(gh_glm_fit_final, gh_predict_dataframe, type = "response")
gh_pred_binary <- ifelse(gh_pred_prob > 0.5, 1, 0)
sum(gh_data$gvhd == gh_pred_binary) #the number of correct predictions
sum(gh_data$gvhd == 1 & gh_pred_binary == 1) / sum(gh_data$gvhd == 1) #sensitivity
sum(gh_data$gvhd == 0 & gh_pred_binary == 0) / sum(gh_data$gvhd == 0) #specificity
```

If we choose the cutoff point as 0.5, 33 cases among 37 case (89%) are correctly predicted.
The sensitivity and the specificity are 0.88 and 0.9, respectively,
which are very high.


```{r}
par(mfrow = c(1, 1))
plot(0, 0, type = "n", xlim = c(0, 1), ylim = c(0, 1),
  xlab = "false positive", ylab = "sensitivity", main="ROC curve")
abline(0, 1, lty=2)

gh_predict_dataframe <- gh_data[, c("dead", "donage", "time", "index")]

for(thres in seq(0, 1, by = 0.005)) {
  gh_pred_prob <- predict(gh_glm_fit_final, gh_predict_dataframe, type = "response")
  gh_pred_binary <- ifelse(gh_pred_prob > thres, 1, 0)
  #data points of malaria data == 100
  sens_ratio <- sum(gh_data$gvhd == 1 & gh_pred_binary == 1) / sum(gh_data$gvhd == 1)
  falsepos_ratio <- sum(gh_data$gvhd == 0 & gh_pred_binary == 1) / sum(gh_data$gvhd == 0)

  points(falsepos_ratio, sens_ratio, pch = 19)
}
```

The ROC curve seems very good.
Again, note that I drew this plot by re-using the training dataset, not test dataset.
This may be a limitation in evaluating the predictive power of this model
by the above ROC curve, too.


Finally, here are the residual plots.
I will draw various kinds of residual plots, 
with setting the x-axis as not only fitted value but also predictors.

```{r}
gh_pearson_res <- residuals(gh_glm_fit_final, type = "pearson")
gh_standardized_res <- rstandard(gh_glm_fit_final)
gh_student_res <- rstudent(gh_glm_fit_final)
gh_dev_res <- residuals(gh_glm_fit_final, type = "deviance")

res_plot_func <- function(res, main){
  par(mfrow = c(2, 2))
  plot(gh_data$donage, res, main = main); abline(h = 0)
  plot(gh_data$time, res); abline(h = 0)
  plot(gh_data$index, res); abline(h = 0)
  plot(gh_pred_prob, res, main = "(verse fitted probability)"); abline(h = 0)
}
res_plot_func(gh_pearson_res, main = "pearson residual")
res_plot_func(gh_standardized_res, main = "standardized residual")
res_plot_func(gh_student_res, main = "jacknife residual")
res_plot_func(gh_dev_res, main = "deviance residual")
```

It seems a little strange when drawn with the fitted probability axis, 
but it seems inevitable due to the nature of logistic regression.
Everything else seems fine.



### \noindent Problem2.

**Do the problem below from James et al. (Problem 4.10 (a)-(d)):**

**This question should be answered using the Weekly data set, which is part of the ISLR package.**
**This data is similar in nature to the Smarket data from this chapter’s lab,**
**except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.**

```{r}
library(ISLR)
# ?Weekly
head(Weekly)
is.factor(Weekly$Direction)
```

According to the R help file, 
The 'Weekly' dataset is about Weekly percentage returns for the S&P 500 stock index between 1990 and 2010.
The number of observation is 1089, and there are 9 variables.

- Year: The year that the observation was recorded
- Lag1: Percentage return for previous week
- Lag2: Percentage return for 2 weeks previous
- Lag3: Percentage return for 3 weeks previous
- Lag4: Percentage return for 4 weeks previous
- Lag5: Percentage return for 5 weeks previous
- Volume: Volume of shares traded (average number of daily shares traded in billions)
- Today: Percentage return for this week
- Direction: A factor with levels Down and Up indicating whether the market had a positive or negative return on a given week

**(a)  Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?**

```{r}
summary(Weekly)
```

Interestingly, the summary statistic of all Lag variables are very similar.
Presumably, during that time period, the S&P market didn't see huge weekly price movements.
Alternatively, this data may have been normalized, 
but according to the data description, it seems more likely that it is not.


```{r}
pairs(Weekly)
```

Between each Lag variable, it looks as if the data is clustered in a circle.
When you look at the year and each lag, it looks like a horizontal line simply because the points are placed around 0.
Year and volume look like a quadratic or exponential curve.

Next, let's look at the relationship in detail between 'Direction' and other variables.

```{r}
par(mfrow = c(2, 4))
plot(Direction~ Year, data = Weekly, pch = 19)
plot(jitter(as.numeric(Direction), 0.8) ~ Lag1, data = Weekly, pch = 19)
plot(jitter(as.numeric(Direction), 0.8) ~ Lag2, data = Weekly, pch = 19)
plot(jitter(as.numeric(Direction), 0.8) ~ Lag3, data = Weekly, pch = 19)
plot(jitter(as.numeric(Direction), 0.8) ~ Lag4, data = Weekly, pch = 19)
plot(jitter(as.numeric(Direction), 0.8) ~ Lag5, data = Weekly, pch = 19)
plot(jitter(as.numeric(Direction), 0.8) ~ Volume, data = Weekly, pch = 19)
plot(jitter(as.numeric(Direction), 0.8) ~ Today, data = Weekly, pch = 19)
```

Unfortunately, to the eye, except for the 'Today' variable, 
the rest variables do not seem to have a significant relationship with 'direction'.
In the case of 'today' variable, it seems to separate the 'direction' value perfectly.



**(b)  Use the full data set to perform a logistic regression with Direction as the response**
**and the five lag variables plus Volume as predictors.**
**Use the summary function to print the results.**
**Do any of the predictors appear to be statistically significant? If so, which ones?**

Now, our model is

\[log{\frac{P(y_i = 1|x_{1i}, x_{2i}, x_{3i}, x_{4i}, x_{5i}, x_{6i})}{P(y_i = 0|x_{1i}, x_{2i}, x_{3i}, x_{4i}, x_{5i}, x_{6i})}}
= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta3 x_{3i} + \beta4 x_{4i} + \beta5 x_{5i} + \beta6 x_{6i}\]
for $i = 1, 2, ..., 1089$, where

where

- $x_{hi}$, "Lag'h'" value of the i-th observation, for $h=1,2,3,4,5$
- $x_{6i}$, 'Volume' value of the i-th observation
- $y_i$: 'direction' value of the i-th observation

```{r}
weekly_glm_fit_full <- glm(
  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
  data = Weekly, family = "binomial")
summary(weekly_glm_fit_full)
```

Under the significance level 0.05, only the 'Lag2' is significant.
If we use other significance levels less than 0.0206, all variables are not significant.


Note that, if we do the chi-square test using the difference of deviances,

H0 : $\beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = \beta_6 = 0$

H1 : at least one parameter in H0 is not 0.

The test statistic is $\chi^2 = 1496.2 - 1486.4 = 9.8$ and follows the chi-square distribution with df $1088-1082 = 6$,
so 
```{r}
1 - pchisq(1496.2 - 1486.4, 1088 - 1082)
```
the p-value is 0.13333.
Thus, we cannot reject the H0 under common significance levels like 0.05 or 0.01,
and we conclude that this model is not statistically significant.
So even if the 'Lag2' variable is significant in individual tests, it may be meaningless.



**(c)  Compute the confusion matrix and overall fraction of correct predictions.**
**Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.**

Let's make the confusion matrix. To find the best cutoff value, 
I calculate the numbers of correct prediction from 0 to 1 by 0.01.
After that, I show the confusion matrix at the cutoff values ​​that maximize the most accurate predictions.

```{r}
weekly_glm_fit_full_prob <- predict(weekly_glm_fit_full, type = "response")

best_score <- 0
best_cutoff <- 0
for(i in seq(0, 1, by = 0.01)){
  weekly_glm_fit_full_pred <- ifelse(weekly_glm_fit_full_prob > i, "Up", "Down")
  score <- sum(weekly_glm_fit_full_pred == Weekly$Direction)
  if(score > best_score) {
    best_cutoff <- i
    best_score <- score
  }
}
weekly_glm_fit_full_pred <- ifelse(weekly_glm_fit_full_prob > best_cutoff, "Up", "Down")
weekly_glm_fit_full_confusion_table <- table(
    pred = weekly_glm_fit_full_pred, true = Weekly$Direction)
cat("best_cutoff:", best_cutoff, "\n")
weekly_glm_fit_full_confusion_table
```

The best cutoff value is 0.49. 
The diagonal elements are correct predictions, and off-diagnoal elements are incorrect predictions.
So, here, only $575 + 40 = 615$ cases are correctly predicted among 1089 cases.
If we calculate the accuracy ratio, is about 56.47%, which is too low.
Even under the best threshold, and even if this is not a 'test error' but 'training error',
this model is poor to predict 'down' correctly.

From this result, we can conclude that this model is of little value.
It's better to set a new model.

Not only that, for better prediction performance testing,
it is also a good idea to split the training and test set.



**(d)  Now fit the logistic regression model using a training data period from 1990 to 2008,**
**with Lag2 as the only predictor.**
**Compute the confusion matrix and the overall fraction of correct predictions for the held out data**
**(that is, the data from 2009 and 2010).**

Under the same notation above, our model is

\[log{\frac{P(y_i = 1|x_{2i})}{P(y_i = 0|x_{2i})}}
= \beta_0 + \beta_2 x_{2i}\]
for $i = 1, 2, ..., 1089$

Let's fit this new model after splitting the dataset to training data and test data.
Now, the number of observations are 985 in the training dataset,
and 104 in the test dataset.

```{r}
head(Weekly)
Weekly_train <- subset(Weekly, Year <= 2008)
Weekly_test <- subset(Weekly, Year >= 2009)

weekly_glm_fit_lag2 <- glm(
  Direction ~ Lag2, data = Weekly_train, family = "binomial")
summary(weekly_glm_fit_lag2)
```

Using the difference of two deviances, we can test 

H0: $\beta_2 = 0$

H1: $\beta_2 \neq 0$

The test statistic is $\chi^2 = 1354.7 - 1350.5 = 4.2$, and under H0, $\chi^2$ follows the chi-square distribution with df 1.


```{r}
1 - pchisq(1354.7 - 1350.5, 1)
```

The p-value is 0.0404, so we can reject the H0 under the significance level 0.05.
(but we cannot reject the H0 if we use the level 0.01.)

Note that In the R output, the result of wald-type test (t-test) also shows that the 'Lag2' variable is significant
under the significance level 0.05.


Let's make the confusion table using the test dataset.

```{r}
weekly_glm_fit_lag2_prob <- predict(weekly_glm_fit_lag2, Weekly_test, type = "response")

best_score <- 0
best_cutoff <- 0
for(i in seq(0, 1, by = 0.01)){
  weekly_glm_fit_lag2_pred <- ifelse(weekly_glm_fit_lag2_prob > i, "Up", "Down")
  score <- sum(weekly_glm_fit_lag2_pred == Weekly_test$Direction)
  if(score > best_score) {
    best_cutoff <- i
    best_score <- score
  }
}
weekly_glm_fit_lag2_pred <- ifelse(weekly_glm_fit_lag2_prob > best_cutoff, "Up", "Down")
weekly_glm_fit_lag2_confusion_table <- table(
    pred = weekly_glm_fit_lag2_pred, true = Weekly_test$Direction)
cat("best_cutoff:", best_cutoff, "\n")
weekly_glm_fit_lag2_confusion_table
```

Coincidentally, the best cutoff point is 0.49 again.
Among 104 points of the test dataset, $7 + 58 = 65$ cases (62.5%) are correctly predicted.
It's not very satisfactory, but the existing model has been improved.

We can additionally try to draw ROC plot or do residual analysis, but I will omit them here
since they were not asked in the question.



### \noindent Problem3.

**Do the problem below from James et al. Problem 4.11: (a), (b), (c) (explain how you partitioned the data) and (f)):**

**In this problem, you will develop a model to predict whether a given car gets high or low gas mileage**
**based on the Auto data set.**

```{r}
# auto <- read.csv("ItSL-data/Auto.csv")
library(ISLR)
# ?Auto
auto <- Auto
auto$origin <- factor(auto$origin)
head(auto)
summary(auto)
```
The 'Auto.csv' and the 'Auto' dataset in ISLR package are exactly the same.
In order to get information about the dataset (using '?Auto') and 
to properly match the data type for each variable, I just use the ISLR package.
(But I found that the 'origin' is not factor in the ISLR::Auto.
So I manually cast the variable to the factor.)

According to the R help file about 'Auto',
there are $408 - 16 = 392$ observation (16 are missing), and each variable is

- mpg: miles per gallon
- cylinders: Number of cylinders between 4 and 8
- displacement: Engine displacement (cu. inches)
- horsepower: Engine horsepower
- weight: Vehicle weight (lbs.)
- acceleration: Time to accelerate from 0 to 60 mph (sec.)
- year: Model year (modulo 100)
- origin: Origin of car (1. American, 2. European, 3. Japanese)
- name: Vehicle name

Let's do EDA.

```{r}
pairs(auto[, -9])
```

Because we will do not use the 'mpg' variable itself but a new binary variable created from the 'mpg',
I will skip elaborating the relationship between 'mpg' and other variables even if
we can see that there are interesting relationships.

We can see some linear relationships among 'cylinders', 'displacement', 'horsepower', 'weight', and 'acceleration'.
If we use a model that can be affected by colinearity, we must be careful.


**(a) and (b): Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median,**
**and a 0 if mpg contains a value below its median.**
**You can compute the median using the median() function.**
**Note you may find it helpful to use the data.frame() function to create a single data set**
**containing both mpg01 and the other Auto variables.**

**Explore the data graphically in order to investigate the association between mpg01 and the other features.**
**Which of the other features seem most likely to be useful in predicting mpg01?**
**Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.**

After making the binary variable 'mpg01', let's see some relationship between 'mpg01' and other variables
by drawing figures.

```{r}
(mpg_median <- median(auto$mpg))
auto$mpg01 <- ifelse(auto$mpg > mpg_median, 1, 0)
```
```{r}
par(mfrow = c(3, 3))
plot(jitter(mpg01, 1) ~ jitter(cylinders, 1), data = auto)
plot(jitter(mpg01, 1) ~ jitter(displacement, 5), data = auto)
plot(jitter(mpg01, 1) ~ jitter(horsepower, 1), data = auto)
plot(jitter(mpg01, 1) ~ weight, data = auto)
plot(jitter(mpg01, 1) ~ acceleration, data = auto)
plot(jitter(mpg01, 1) ~ jitter(year, 1), data = auto)
plot(jitter(mpg01, 1) ~ jitter(as.numeric(origin), 1), data = auto)
```
We can find that if one of the cylinder, displacement, horsepower, or weight value increases,
the value of 'mpg01' tends to be 0.
And if the accelerate value increase, the value of 'mpg01' is likely to become 1.

Note that these 5 variables show linearity in the EDA procedure above (see pair plot again!).
First 4 variables are correlated in the same direction, and accelerate is conversly related.
It seems better to use only one of these variable because of the colinearity if we want to include them.
Putting several highly correlated variables to our model may make a convergence problem
for the logistic regression fitting algorithm.

So, we can conclude that it will be useful to use one of these five variable to predict mpg01.
I think we can get a model with similar performance by choosing any one of the five.

When I see only with my eyes, I think 'cylinders' is the best choice.
However, I will pick 'weight' because it is continuous variable.
I think this choice may make interpretation or prediction more interesting.


**(c) Split the data into a training set and a test set.**

I will put 40 (about 10%) of data points as a test set.
After I randomly select a index for the test set using the 'sample' function, let's split the 'Auto' dataset.
(For reproducibility, I will set the seed value right above the sample function.)

```{r}
set.seed(20211126)
test_index <- sort(sample(1:dim(auto)[1], 40))
train_index <- setdiff(1:dim(auto)[1], test_index)

auto_train <- auto[train_index,]
auto_test <- auto[test_index, ]
dim(auto_train)
dim(auto_test)
```

Now, the training set consists of 352 data points.


**(f) Perform logistic regression on the training data in order to predict mpg01 using the variables**
**that seemed most associated with mpg01 in (b). What is the test error of the model obtained?**

add model notation 

```{r}
auto_glm_fit <- glm(mpg01 ~ weight, data = auto_train)
summary(auto_glm_fit)
```

Deviance
```{r}
87.955 - 38.371
1 - pchisq(87.955 - 38.371, 1)
```
Very good!

coeff intepretation

prob example table
```{r}
logit_inv <- function(z){
  return(exp(z) / (1 + exp(z)))
}

auto_coeff <- auto_glm_fit$coefficients
library(knitr)
kable(data.frame(
  weight = seq(1000, 5000, by = 500),
  probability = logit_inv(
    auto_coeff[1] + auto_coeff[2] * seq(1000, 5000, by = 500)
    )
  )
)
```

We can draw the fitted curve.

```{r}
par(mfrow = c(1, 1))
plot(mpg01 ~ weight, pch = 19, data = auto_train,
  xlim = c(1000, 5500), ylab = "mpg01", xlab = "weight")
curve(logit_inv(auto_coeff[1] + auto_coeff[2] * x), add = TRUE, col = "red")
```


Evaluation:

```{r}

auto_glm_fit_prob <- predict(auto_glm_fit, auto_test, type = "response")

cutoff_candidates <- seq(0, 1, by = 0.01)
best_score <- 0
best_cutoff <- 0
sens_ratio <- rep(0, length(cutoff_candidates))
falsepos_ratio <- rep(0, length(cutoff_candidates))

for (i in seq_len(length(cutoff_candidates))) {
  cutoff_val <- cutoff_candidates[i]
  auto_glm_pred_binary <- ifelse(auto_glm_fit_prob > cutoff_val, 1, 0)
  score <- sum(auto_glm_pred_binary == auto_test$mpg01)
  if (score > best_score) {
    best_cutoff <- cutoff_val
    best_score <- score
  }
  sens_ratio[i] <- sum(auto_test$mpg01 == 1 & auto_glm_pred_binary == 1) / sum(auto_test$mpg01 == 1)
  falsepos_ratio[i] <- sum(auto_test$mpg01 == 0 & auto_glm_pred_binary == 1) / sum(auto_test$mpg01 == 0)
}

auto_glm_pred_binary <- ifelse(auto_glm_fit_prob > best_cutoff, 1, 0)
auto_glm_confusion_table <- table(
    pred = auto_glm_pred_binary, true = auto_test$mpg01)
cat("best_cutoff:", best_cutoff, "\n")
auto_glm_confusion_table

plot(falsepos_ratio, sens_ratio,
  type = "l", main = "ROC curve",
  xlim = c(0, 1), ylim = c(0, 1))
```


residual plots for fitted

```{r}
auto_pearson_res <- residuals(auto_glm_fit, type = "pearson")
auto_standardized_res <- rstandard(auto_glm_fit)
auto_student_res <- rstudent(auto_glm_fit)
auto_dev_res <- residuals(auto_glm_fit, type = "deviance")
auto_fitted_prob <- logit_inv(auto_coeff[1] + auto_coeff[2] * auto_train$weight)

res_plot_func <- function(res, main){
  par(mfrow = c(1, 2))
  plot(auto_train$weight, res, main = main)
  abline(h = 0)
  plot(auto_fitted_prob, res, main = "(verse fitted probability)")
  abline(h = 0)
}
res_plot_func(auto_pearson_res, main = "pearson residual")
res_plot_func(auto_standardized_res, main = "standardized residual")
res_plot_func(auto_student_res, main = "jacknife residual")
res_plot_func(auto_dev_res, main = "deviance residual")
```

