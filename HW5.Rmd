---
title: "Homework Assignment 5"
author: "Seokjun Choi"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

### \noindent Problem1.

**Fit a logistic regression model to the graft.vs.host data from the ISwR package**
**predicting gvhd response.**
**Use different transformations of the index variable.**

```{r}
library(ISwR)
gh_data <- graft.vs.host
# ?graft.vs.host

gh_data$type <- factor(gh_data$type)
gh_data$preg <- factor(gh_data$preg)
gh_data$gvhd <- factor(gh_data$gvhd)
gh_data$dead <- factor(gh_data$dead)

head(gh_data)
```

According to the R help file of 'graft.vs.host' data,
these data are about "Graft versus host disease" and its predictors.
The dataset consists of 37 subjects, 9 variables (1 response, 1 subject index ('pnr'), 7 predictors).
Each column means

- pnr: a numeric vector patient number.
- rcpage: a numeric vector, age of recipient (years).
- donage: a numeric vector, age of the donor (years).
- type: factor, type of leukaemia coded 1: AML, 2: ALL, 3: CML for acute myeloid, acute lymphatic, and chronic myeloid leukaemia.
- preg: factor, indicating whether donor has been pregnant. 0: no, 1: yes.
- index: a numeric vector giving an index of mixed epidermal cell-lymphocyte reactions.
- gvhd: factor, graft-versus-host disease, 0: no, 1: yes.
- time: a numeric vector, follow-up time
- dead: factor, 0: no (censored), 1: yes

Let's do EDA.

```{r}
## put (absolute) correlations on the upper panels,
## with size proportional to the correlations.
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...){
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

pairs(gh_data[, -1], lower.panel = panel.smooth, upper.panel = panel.cor,
      gap=0, row1attop=FALSE) #exclude 'pnr'
```

Two pairs of variables have linear relations. (between 'dead' and 'time', and between 'donage' and 'rcpage'.)

Additionally, Let's look at relationships between 'gvhd' and other variables in detail.

```{r, fig.height=3}
par(mfrow = c(1, 3))
plot(jitter(as.numeric(gvhd), amount = 0.1) ~ rcpage,
  data = gh_data, pch = 19, ylab = "gvhd")
plot(jitter(as.numeric(gvhd), amount = 0.1) ~ donage,
  data = gh_data, pch = 19, ylab = "gvhd")
plot(jitter(as.numeric(gvhd), amount = 0.1) ~ time,
  data = gh_data, pch = 19, ylab = "gvhd")
plot(gvhd ~ type, data = gh_data)
plot(gvhd ~ preg, data = gh_data)
plot(gvhd ~ dead, data = gh_data)
```

If values of two continuous variables 'rcpage' and 'donage' are higher,
it seems that the possibility of having graft-versus-host disease tends to increase.
In the case of another continuous variable 'time', the chance of g-v-h disease seems less
when the time increase.

Next, let's look at the 'index' variable, which is mentioned specifically in the problem.
To choose a good transformation for 'index' variable,
I draw four figures with
log-transform ($\sim x^0$), square root transform($\sim x^{1/2}$),
data without transform($x^1$), and square transform($x^2$), respectively.

```{r}
par(mfrow = c(2, 2))
plot(jitter(as.numeric(gvhd), amount = 0.1) ~ log(index),
  data = gh_data, pch = 19, ylab = "gvhd")
plot(jitter(as.numeric(gvhd), amount = 0.1) ~ sqrt(index),
  data = gh_data, pch = 19, ylab = "gvhd")
plot(jitter(as.numeric(gvhd), amount = 0.1) ~ index,
  data = gh_data, pch = 19, ylab = "gvhd")
plot(jitter(as.numeric(gvhd), amount = 0.1) ~ (index)^2,
  data = gh_data, pch = 19, ylab = "gvhd")
```

Among four cases, the log-transform case seems the best.
This is because the 'gvhd' seems to be differentiated better
by the log-'index' value than in other cases.
Thus, I choose the log transform as the transform of 'index' variable.

I will exclude all interaction terms.
This is simply because I want to avoid complex models.
(I may try to justify this decision more by looking at the EDA step
by drawing all pairs of marginal-joint plots with 'gvhd' and other two predictors.
Or, I may deviance test for model comparison.
However, I will omit these things here because they will be too long.)

Then, our first model is

\[logit(P(y_i = 1 | x_{i1}, x_{i2}, x_{i3}, x_{i4}, x_{i5}, x_{i6}, x_{i7}, x_{i8})) = 
  \beta_0 + \alpha_1 x_{i1} + \alpha_2 x_{i2} + \alpha_3 x_{i3} + \alpha_4 x_{i4}
  + \beta_5 x_{i5} + \beta_6 x_{i6} + \beta_7 x_{i7} + \beta_8 log{x_{i8}} \]
for i = 1,2,...,37, where

- $y_i$: (binary factor) 1 if i-th patient has graft-versus-host disease, 0 otherwise
- $x_{i1}$: (binary factor) 1 if i-th donor has been pregnant. 0 otherwise
- $x_{i2}$: (dummy variable for the 3-level factor, 'type') i-th patient's leukaemia type of i-th sample. 0 if AML or CML, 1 if ALL.
- $x_{i3}$: (dummy variable for the 3-level factor, 'type') i-th patient's leukaemia type of i-th sample. 0 if AML or ALL, 1 CML.
- $x_{i4}$: (binary factor) 1 if i-th patient is dead. 0 otherwise (censored)
- $x_{i5}$: i-th data point's age of recipient (years)
- $x_{i6}$: i-th data point's age of donor (years).
- $x_{i7}$: i-th data point's follow up time.
- $x_{i8}$: i-th data point's index of mixed epidermal cell-lymphocyte reactions

Note that, I used dummy-variable notation since there are too many factor variables.
Also note that, because 'type' factor has three levels, there are two terms for 'type' variable in this model.
($x_{i2} and x_{i3}$)

```{r}
gh_glm_fit_full <- glm(
  gvhd ~ type + preg + dead + rcpage + donage + time + log(index),
  data = gh_data, family = binomial)
summary(gh_glm_fit_full)
```

Unfortunately, the numerical algorithm to fit this model failed.
I cannot be sure the exact reason, but one possible guess is the colinearity between the predictors.
After some experiments, I found that convergence fails if 'time', 'type', and 'log(index)' variables are put together in the model.

So, let's go to back to the 'pairs' plot in EDA procedure. 
Let's take a closer look at the relationship between the response variable 'gvhd' and each variable,
and between these two variable.

```{r, fig.height=3}
par(mfrow = c(1, 3))
plot(gvhd ~ time, data = gh_data)
plot(gvhd ~ type, data = gh_data)
plot(time ~ type, data = gh_data)
```

In the third plot, we can again confirm that there is a kind of positive relationship between two predictors (even though their correlation value is low).
However, the patterns in mosaic plots between each predictor and the 'gvhd' are different.
It is a kind of inconsistency, so I guess that it may cause a convergence problem in the numerical method. (I cannot be sure though.)
So, in here, I'll delete two terms related to the 'type' variable ($\alpha_2 x_{i2} and \alpha_3 x_{i3}$).

Then our model becomes

\[logit(P(y_i = 1 | x_{i1}, x_{i4}, x_{i5}, x_{i6}, x_{i7}, x_{i8})) = 
  \beta_0 + \alpha_1 x_{i1} +  \alpha_4 x_{i4}
  + \beta_5 x_{i5} + \beta_6 x_{i6} + \beta_7 x_{i7} + \beta_8 log{x_{i8}} \]
for i = 1,2,...,37, with the same notation as original model,


```{r}
gh_glm_fit_wo_type <- glm(
  gvhd ~ preg + dead + rcpage + donage + time + log(index),
  data = gh_data, family = binomial)
summary(gh_glm_fit_wo_type)
```

Fitting is successful (even though 8 iterations were run).
However, there are still so many insignificant variables in our model
when we think about their t-statistics and p-values.
But it is also known that a wald-type test is poor in the context of logistic regression.
Therefore, after I try to delete some variables,
I will decide whether I include the variable or not in our model using AIC criterion.


Here is a result-sequence using a kind of backward selection.
```{r}
# after delete preg
gh_glm_fit_3 <- glm(gvhd ~ dead + rcpage + donage + time + log(index),
  data = gh_data, family = binomial)
summary(gh_glm_fit_3)

# after delete rcpage
gh_glm_fit_4 <- glm(gvhd ~ dead + donage + time + log(index),
  data = gh_data, family = binomial)
summary(gh_glm_fit_4)

# after delete dead
gh_glm_fit_5 <- glm(gvhd ~ donage + time + log(index),
  data = gh_data, family = binomial)
summary(gh_glm_fit_5)

# after delete donage
gh_glm_fit_6 <- glm(gvhd ~ time + log(index),
  data = gh_data, family = binomial)
summary(gh_glm_fit_6)
```

The lowest AIC was achieved when we fit the 'gh_glm_fit_4'.
So, I will use the model as our final model.

Then, now our model is

\[logit(P(y_i = 1 | x_{i4}, x_{i6}, x_{i7}, x_{i8})) = 
  \beta_0 + \alpha_4 x_{i4}
  + \beta_6 x_{i6} + \beta_7 x_{i7} + \beta_8 log{x_{i8}} \]
for i = 1,2,...,37, with 

- $y_i$: 1 if i-th patient has graft-versus-host disease, 0 otherwise
- $x_{i4}$: 1 if i-th patient is dead. 0 otherwise (censored)
- $x_{i6}$: i-th data point's age of donor (years).
- $x_{i7}$: i-th data point's follow up time.
- $x_{i8}$: i-th data point's index of mixed epidermal cell-lymphocyte reactions

and $\alpha_4, \beta_6, \beta_7, \beta_8$ are effects of these variables, respectively.
I will give exact interpretation of these coefficients below.

The fitting result is,

```{r}
#same as 'gh_glm_fit_4'
gh_glm_fit_final <- glm(gvhd ~ dead + donage + time + log(index),
  data = gh_data, family = binomial)
summary(gh_glm_fit_final)
```

Before interpreting fitted coefficients, let's check whether this model is significant or not.

H0 : $\alpha_4 = \beta_6 = \beta_7 = \beta_8$ = 0$

H1 : at least one parameter in H0 is not 0.

Under the H0, the difference between the null-deviance and the residual deviance
follows the chi-square distribution with degrees of freedom $36-32 = 4$.

```{r}
(gh_glm_chi_statistic <- 51.049 - 15.945)
(gh_glm_chi_p_value <- 1 - pchisq(51.049 - 15.945, 36 - 32))
```

The test statistic value is 35.104, and the p-value of this test is very close to 0.
So under common significance levels like 0.05 or 0.01, we can reject H0.
Thus, this model is statistically significant.

Next, let's look at each coefficient.
When the i-th patient is not dead,
\[\frac{P(y_i = 1 | x_{i4}=0, x_{i6}, x_{i7}, x_{i8})}
    {P(y_i = 0 | x_{i4}=0, x_{i6}, x_{i7}, x_{i8})} = 
  exp(-0.68 + 0.21 x_{i6} - 0.01 x_{i7} + 5.08 log{x_{i8}})\]

Otherwise, if the i-th patient is dead,
\[\frac{P(y_i = 1 | x_{i4}=1, x_{i6}, x_{i7}, x_{i8})}
    {P(y_i = 0 | x_{i4}=1, x_{i6}, x_{i7}, x_{i8})} = 
  exp(-0.68 -3.70 + 0.21 x_{i6} - 0.01 x_{i7} + 5.08 log{x_{i8}})\]

Each coefficient can be interpreted as the (multiplicative) effect to the odds by the one-unit increased predictor.
For example, if the log-transformed index increase 1 unit (or, in other words, if the index value get multiplied by $e$,)
and if other variables stay fixed value at the same time,
the odds would be multiplied by $e^{5.08}$.
Or equivalently, the odds ratio is
\[\frac{P(Y=1|log{x_8}=c+1)/P(Y=0|log{x_8}=c+1)}{P(Y=1|log{x_8}=c)/P(Y=0|log{x_8}=c)} = e^{5.08}\]
for any $c\in \mathbb{R}$ and under other variables are fixed.


Let's evaluate this fitting result.
A better model evaluation would have been possible if I divided the training and test data from the beginning. 
But I will omit it here since we started without dividing our dataset.
So I note that there is a limitation in the analysis below, 
because the evaluation using prediction is only the evaluation of the training error. 

```{r}
gh_predict_dataframe <- gh_data[, c("dead", "donage", "time", "index")]
gh_pred_prob <- predict(gh_glm_fit_final, gh_predict_dataframe, type = "response")
gh_pred_binary <- ifelse(gh_pred_prob > 0.5, 1, 0)
sum(gh_data$gvhd == gh_pred_binary) #the number of correct predictions
sum(gh_data$gvhd == 1 & gh_pred_binary == 1) / sum(gh_data$gvhd == 1) #sensitivity
sum(gh_data$gvhd == 0 & gh_pred_binary == 0) / sum(gh_data$gvhd == 0) #specificity
```

If we choose the cutoff point as 0.5, 33 cases among 37 case (89%) are correctly predicted.
The sensitivity and the specificity are 0.88 and 0.9, respectively,
which are very high.


```{r}
par(mfrow = c(1, 1))
plot(0, 0, type = "n", xlim = c(0, 1), ylim = c(0, 1),
  xlab = "false positive", ylab = "sensitivity", main="ROC curve")
abline(0, 1, lty=2)

gh_predict_dataframe <- gh_data[, c("dead", "donage", "time", "index")]

for(thres in seq(0, 1, by = 0.005)) {
  gh_pred_prob <- predict(gh_glm_fit_final, gh_predict_dataframe, type = "response")
  gh_pred_binary <- ifelse(gh_pred_prob > thres, 1, 0)
  #data points of malaria data == 100
  sens_ratio <- sum(gh_data$gvhd == 1 & gh_pred_binary == 1) / sum(gh_data$gvhd == 1)
  falsepos_ratio <- sum(gh_data$gvhd == 0 & gh_pred_binary == 1) / sum(gh_data$gvhd == 0)

  points(falsepos_ratio, sens_ratio, pch = 19)
}
```

The ROC curve seems very good.
Again, note that I drew this plot by re-using the training dataset, not test dataset.
This may be a limitation in evaluating the predictive power of this model
by the above ROC curve, too.


Finally, here are the residual plots.
I will draw various kinds of residual plots, 
with setting the x-axis as not only fitted value but also predictors.

```{r, fig.height=4}
gh_pearson_res <- residuals(gh_glm_fit_final, type = "pearson")
gh_standardized_res <- rstandard(gh_glm_fit_final)
gh_student_res <- rstudent(gh_glm_fit_final)
gh_dev_res <- residuals(gh_glm_fit_final, type = "deviance")

res_plot_func <- function(res, main){
  par(mfrow = c(2, 2))
  plot(gh_data$donage, res, main = main); abline(h = 0)
  plot(gh_data$time, res); abline(h = 0)
  plot(gh_data$index, res); abline(h = 0)
  plot(gh_pred_prob, res, main = "(verse fitted probability)"); abline(h = 0)
}
res_plot_func(gh_pearson_res, main = "pearson residual")
res_plot_func(gh_standardized_res, main = "standardized residual")
res_plot_func(gh_student_res, main = "jacknife residual")
res_plot_func(gh_dev_res, main = "deviance residual")
```

It seems a little strange when drawn with the fitted probability axis, 
but it seems inevitable due to the nature of logistic regression.
Everything else seems fine.



### \noindent Problem2.

**Do the problem below from James et al. (Problem 4.10 (a)-(d)):**

**This question should be answered using the Weekly data set, which is part of the ISLR package.**
**This data is similar in nature to the Smarket data from this chapter’s lab,**
**except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.**

```{r}
library(ISLR)
# ?Weekly
head(Weekly)
is.factor(Weekly$Direction)
```

According to the R help file, 
The 'Weekly' dataset is about Weekly percentage returns for the S&P 500 stock index between 1990 and 2010.
The number of observations is 1089, and there are 9 variables.

- Year: The year that the observation was recorded
- Lag1: Percentage return for the previous week
- Lag2: Percentage return for 2 weeks previous
- Lag3: Percentage return for 3 weeks previous
- Lag4: Percentage return for 4 weeks previous
- Lag5: Percentage return for 5 weeks previous
- Volume: Volume of shares traded (average number of daily shares traded in billions)
- Today: Percentage return for this week
- Direction: A factor with levels Down and Up indicating whether the market had a positive or negative return on a given week

**(a)  Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?**

```{r}
summary(Weekly)
```

Interestingly, the summary statistics of all Lag variables are very similar.
Presumably, during that time period, the S&P market didn't show huge weekly price movements.
Alternatively, this data may have been normalized, 
but according to the data description, it seems more likely that it is not.


```{r, fig.height=9}
pairs(Weekly)
```

Between each Lag variable, it looks as if the data is clustered in a circle.
When you look at the year and each lag, it looks like a horizontal line  
because the points are placed around 0.
Year and volume look like a quadratic or exponential curve.

Next, let's look at the relationship in detail between 'Direction' and other variables.

```{r}
par(mfrow = c(2, 4))
plot(Direction~ Year, data = Weekly, pch = 19)
plot(jitter(as.numeric(Direction), 0.8) ~ Lag1, data = Weekly, pch = 19)
plot(jitter(as.numeric(Direction), 0.8) ~ Lag2, data = Weekly, pch = 19)
plot(jitter(as.numeric(Direction), 0.8) ~ Lag3, data = Weekly, pch = 19)
plot(jitter(as.numeric(Direction), 0.8) ~ Lag4, data = Weekly, pch = 19)
plot(jitter(as.numeric(Direction), 0.8) ~ Lag5, data = Weekly, pch = 19)
plot(jitter(as.numeric(Direction), 0.8) ~ Volume, data = Weekly, pch = 19)
plot(jitter(as.numeric(Direction), 0.8) ~ Today, data = Weekly, pch = 19)
```

Unfortunately, to the eye, except for the 'Today' variable, 
the rest variables do not seem to have a significant relationship with 'direction'.
In the case of 'today' variable, it seems to separate the 'direction' value perfectly.



**(b)  Use the full data set to perform a logistic regression with Direction as the response**
**and the five lag variables plus Volume as predictors.**
**Use the summary function to print the results.**
**Do any of the predictors appear to be statistically significant? If so, which ones?**

Now, our model is

\[log{\frac{P(y_i = 1|x_{1i}, x_{2i}, x_{3i}, x_{4i}, x_{5i}, x_{6i})}{P(y_i = 0|x_{1i}, x_{2i}, x_{3i}, x_{4i}, x_{5i}, x_{6i})}}
= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta3 x_{3i} + \beta4 x_{4i} + \beta5 x_{5i} + \beta6 x_{6i}\]
for $i = 1, 2, ..., 1089$, where

where

- $x_{hi}$, "Lag'h'" value of the i-th observation, for $h=1,2,3,4,5$
- $x_{6i}$, 'Volume' value of the i-th observation
- $y_i$: 'direction' value of the i-th observation

```{r}
weekly_glm_fit_full <- glm(
  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
  data = Weekly, family = "binomial")
summary(weekly_glm_fit_full)
```

Under the significance level 0.05, only the 'Lag2' is significant (under a wald-type test.)
But if we use other significance levels less than 0.0206 (for example, 0.01), 
all variables are not significant.


Note that, if we do the chi-square test using the difference of deviances, or specifically

H0 : $\beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = \beta_6 = 0$

H1 : at least one parameter in H0 is not 0.

then the test statistic is $\chi^2 = 1496.2 - 1486.4 = 9.8$ and follows the chi-square distribution with df $1088-1082 = 6$,
so the p-value is
```{r}
1 - pchisq(1496.2 - 1486.4, 1088 - 1082)
```

We cannot reject the H0 under typical significance levels like 0.05 or 0.01,
and we conclude that this model is not statistically significant.
So even if the 'Lag2' variable is significant in individual tests, it may be meaningless.

As a result, we can say that all predictor's effects are statistically 0.
For this reason, I will not interpret the coefficients.


**(c)  Compute the confusion matrix and overall fraction of correct predictions.**
**Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.**

Let's make the confusion matrix. To find the best cutoff value, 
I calculate the number of correct predictions for each cutoff value from 0 to 1 every 0.01 units.
After that, I show the confusion matrix at the cutoff values ​​that maximize the number of accurate predictions.

```{r}
weekly_glm_fit_full_prob <- predict(weekly_glm_fit_full, type = "response")

best_score <- 0
best_cutoff <- 0
for(i in seq(0, 1, by = 0.01)){
  weekly_glm_fit_full_pred <- ifelse(weekly_glm_fit_full_prob > i, "Up", "Down")
  score <- sum(weekly_glm_fit_full_pred == Weekly$Direction)
  if(score > best_score) {
    best_cutoff <- i
    best_score <- score
  }
}
print(best_cutoff)
```

The best cutoff value is 0.49. 

```{r}
weekly_glm_fit_full_pred <- ifelse(weekly_glm_fit_full_prob > best_cutoff, "Up", "Down")
weekly_glm_fit_full_confusion_table <- table(
    pred = weekly_glm_fit_full_pred, true = Weekly$Direction)
cat("best_cutoff:", best_cutoff, "\n")
weekly_glm_fit_full_confusion_table
```

The diagonal elements are correct predictions, and off-diagonal elements are incorrect predictions.
Here, only $575 + 40 = 615$ cases are correctly predicted among 1089 cases.
If we calculate the accuracy ratio, is about 56.47%, which is too low, even under the best threshold.
Especially, this model has shown bad performance to predict 'down' correctly.

Note that this result is from not 'test data' but 'training data.'
In other words, this model could not even predict properly the data used by its fitting.
As a result, we can conclude that this model is poor.
It's better to set a new model.


**(d)  Now fit the logistic regression model using a training data period from 1990 to 2008,**
**with Lag2 as the only predictor.**
**Compute the confusion matrix and the overall fraction of correct predictions for the held out data**
**(that is, the data from 2009 and 2010).**

Under the same notation above, our model is

\[log{\frac{P(y_i = 1|x_{2i})}{P(y_i = 0|x_{2i})}}
= \beta_0 + \beta_2 x_{2i}\]
for $i = 1, 2, ..., 1089$

Let's fit this new model after splitting the dataset into training data and test data.
I split the data as instructed in the problem.
Now, the observations are 985 in the training dataset and 104 in the test dataset.

```{r}
head(Weekly)
Weekly_train <- subset(Weekly, Year <= 2008)
Weekly_test <- subset(Weekly, Year >= 2009)

weekly_glm_fit_lag2 <- glm(
  Direction ~ Lag2, data = Weekly_train, family = "binomial")
summary(weekly_glm_fit_lag2)
```

Using the difference of two deviances, we can test 

H0: $\beta_2 = 0$

H1: $\beta_2 \neq 0$

The test statistic is $\chi^2 = 1354.7 - 1350.5 = 4.2$, 
and under H0, $\chi^2$ follows the chi-square distribution with df 1.


```{r}
1 - pchisq(1354.7 - 1350.5, 1)
```

The p-value is 0.0404, so we can reject the H0 under the significance level of 0.05.
(but we cannot reject the H0 if we use the level of 0.01.)

Note that In the R output, the result of a wald-type test (t-test) also shows that the 'Lag2' variable is significant
under the significance level of 0.05.


Let's make the confusion table using the test dataset.

```{r}
weekly_glm_fit_lag2_prob <- predict(weekly_glm_fit_lag2, Weekly_test, type = "response")

best_score <- 0
best_cutoff <- 0
cutoff_candidates <- seq(0, 1, by = 0.01)
sens_ratio <- rep(0, length(cutoff_candidates))
falsepos_ratio <- rep(0, length(cutoff_candidates))

for(i in seq_along(cutoff_candidates)) {
  now_cutoff <- cutoff_candidates[i]
  weekly_glm_fit_lag2_pred <- ifelse(weekly_glm_fit_lag2_prob > now_cutoff, "Up", "Down")
  score <- sum(weekly_glm_fit_lag2_pred == Weekly_test$Direction)
  if(score > best_score) {
    best_cutoff <- now_cutoff
    best_score <- score
  }
  sens_ratio[i] <- sum(Weekly_test$Direction == "Up" & weekly_glm_fit_lag2_pred == "Up") / sum(Weekly_test$Direction == "Up")
  falsepos_ratio[i] <- sum(Weekly_test$Direction == "Down" & weekly_glm_fit_lag2_pred == "Up") / sum(Weekly_test$Direction == "Down")
}
print(best_cutoff)
```

Coincidentally, the best cutoff point is 0.49 again.

```{r}
weekly_glm_fit_lag2_pred <- ifelse(weekly_glm_fit_lag2_prob > best_cutoff, "Up", "Down")
weekly_glm_fit_lag2_confusion_table <- table(
    pred = weekly_glm_fit_lag2_pred, true = Weekly_test$Direction)
cat("best_cutoff:", best_cutoff, "\n")
weekly_glm_fit_lag2_confusion_table
```

Among 104 points of the test dataset, $7 + 58 = 65$ cases (62.5%) are correctly predicted.
It's not satisfactory, but the model has been improved compared to problem (b) and (c).


Additionally, let's try to draw ROC plot using the test set.

```{r}
par(mfrow = c(1, 1))
plot(falsepos_ratio, sens_ratio,
  type = "l", main = "ROC curve",
  xlim = c(0, 1), ylim = c(0, 1)
)
abline(0, 1)
```

Shockingly, the ROC curve is below the diagonal in some parts!
In other words, this model sometimes shows worse performance than just completely random prediction
under some cutoff points.

Finally, here are residual plots.

```{r, fig.height=3.5}
weekly_pearson_res <- residuals(weekly_glm_fit_lag2, type = "pearson")
weekly_standardized_res <- rstandard(weekly_glm_fit_lag2)
weekly_student_res <- rstudent(weekly_glm_fit_lag2)
weekly_dev_res <- residuals(weekly_glm_fit_lag2, type = "deviance")
weekly_coeff <- weekly_glm_fit_lag2$coefficients

logit_inv <- function(z){
  return(exp(z) / (1 + exp(z)))
}

weekly_fitted_prob <- logit_inv(weekly_coeff[1] + weekly_coeff[2] * Weekly_train$Lag2)

res_plot_func <- function(res, main){
  par(mfrow = c(1, 2))
  plot(Weekly_train$Lag2, res, main = main)
  abline(h = 0)
  plot(weekly_fitted_prob, res, main = "")
  abline(h = 0)
}
res_plot_func(weekly_pearson_res, main = "pearson residual")
res_plot_func(weekly_standardized_res, main = "standardized residual")
res_plot_func(weekly_student_res, main = "jacknife residual")
res_plot_func(weekly_dev_res, main = "deviance residual")
```

We can find two lines for each residual plot.
In logistic regression, I think this pattern is common.



### \noindent Problem3.

**Do the problem below from James et al. Problem 4.11: (a), (b), (c) (explain how you partitioned the data) and (f)):**

**In this problem, you will develop a model to predict whether a given car gets high or low gas mileage**
**based on the Auto data set.**

```{r}
# auto <- read.csv("ItSL-data/Auto.csv")
library(ISLR)
# ?Auto
auto <- Auto
auto$origin <- factor(auto$origin)
head(auto)
```

The 'Auto.csv' and the 'Auto' dataset in ISLR package are exactly the same.
In order to get information about the dataset (using '?Auto') and 
to properly match the data type for each variable, I just use the ISLR package.
(But I found that the 'origin' is not a factor in the ISLR::Auto.
So I manually cast the variable to the factor.)

According to the R help file about 'Auto',
there are $408 - 16 = 392$ observation (16 are missing), and each variable is

- mpg: miles per gallon
- cylinders: Number of cylinders between 4 and 8
- displacement: Engine displacement (cu. inches)
- horsepower: Engine horsepower
- weight: Vehicle weight (lbs.)
- acceleration: Time to accelerate from 0 to 60 mph (sec.)
- year: Model year (modulo 100)
- origin: Origin of the car (1. American, 2. European, 3. Japanese)
- name: Vehicle name

Let's do EDA.

```{r, fig.height=8}
summary(auto)
pairs(auto[, -9])
```

Because we will do not use the 'mpg' variable itself but a new binary variable created from the 'mpg',
I will skip elaborating the relationship between 'mpg' and other variables even if
we can see that there are interesting relationships.

We can see some linear relationships among 'cylinders', 'displacement', 'horsepower', 'weight', and 'acceleration'.
We must be careful if we use a model that can be affected by colinearity, like logistic regression models.


**(a) and (b): Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median,**
**and a 0 if mpg contains a value below its median.**
**You can compute the median using the median() function.**
**Note you may find it helpful to use the data.frame() function to create a single data set**
**containing both mpg01 and the other Auto variables.**

**Explore the data graphically in order to investigate the association between mpg01 and the other features.**
**Which of the other features seem most likely to be useful in predicting mpg01?**
**Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.**

After making the binary variable 'mpg01', let's see some relationship between 'mpg01' and other variables
by drawing figures.

```{r}
(mpg_median <- median(auto$mpg))
auto$mpg01 <- ifelse(auto$mpg > mpg_median, 1, 0)
```
```{r, fig.height=8}
par(mfrow = c(3, 3))
plot(jitter(mpg01, 1) ~ jitter(cylinders, 1), data = auto)
plot(jitter(mpg01, 1) ~ jitter(displacement, 5), data = auto)
plot(jitter(mpg01, 1) ~ jitter(horsepower, 1), data = auto)
plot(jitter(mpg01, 1) ~ weight, data = auto)
plot(jitter(mpg01, 1) ~ acceleration, data = auto)
plot(jitter(mpg01, 1) ~ jitter(year, 1), data = auto)
plot(jitter(mpg01, 1) ~ jitter(as.numeric(origin), 1), data = auto)
```

We can find that if one of the 'cylinder', 'displacement', 'horsepower', or 'weight' value increases,
the value of 'mpg01' tends to be 0.
And if the 'accelerate' value increase, the value of 'mpg01' is likely to become 1.

Note that these 5 variables show linearity in the EDA procedure above (see pair plot again!).
First 4 variables are correlated in the same direction, and accelerate is conversly related.
It seems better to use only one of these variable because of the colinearity if we want to include them.
Putting several highly correlated variables to our model may make a convergence problem
for the logistic regression fitting algorithm.

So, we can conclude that it will be useful to use one of these five variables to predict mpg01.
I think we can get a model with a similar performance by choosing any one of the five.

When I see only with my eyes, I think the 'cylinders' variable is the best choice.
(Actually, if we pick just one variable with AIC criterion,
the model 'pmg01~cylinders' gives us the lowest AIC value.)
However, I will pick 'weight' because it is continuous variable.
I expect that this choice may make interpretation or prediction more interesting.


**(c) Split the data into a training set and a test set.**

I will put 40 (about 10%) of data points as a test set.
After I randomly select an index for the test set using the 'sample' function, let's split the 'Auto' dataset.
(For reproducibility, I will set the seed value right above the sample function.)

```{r}
set.seed(20211126)
test_index <- sort(sample(1:dim(auto)[1], 40))
train_index <- setdiff(1:dim(auto)[1], test_index)

auto_train <- auto[train_index, ]
auto_test <- auto[test_index, ]
dim(auto_train)
dim(auto_test)

```

Now, the training set consists of 352 data points.
Let's compare the shapes of each variable in the training and test set.
The left side is the training set, and the right is the test set.

```{r, fig.height=3, echo=FALSE}
hist_train_vs_test <- function(train_set, test_set, var_name) {
  train_set <- as.numeric(train_set)
  test_set <- as.numeric(test_set)
  par(mfrow = c(1, 2))
  upper_bd <- max(range(train_set)[2], range(test_set)[2])
  lower_bd <- min(range(train_set)[1], range(test_set)[1])
  set_breaks <- seq(lower_bd, upper_bd, length.out = 11)
  train_hist <- hist(train_set,
    xlab = paste("train:", var_name), main = "",
    breaks = set_breaks)
  hist(test_set,
    breaks = set_breaks,
    xlab = paste("test:", var_name), main = "")
}

hist_train_vs_test(auto_train$mpg, auto_test$mpg, "mpg")
hist_train_vs_test(auto_train$cylinders, auto_test$cylinders, "cylinders")
hist_train_vs_test(auto_train$displacement, auto_test$displacement, "displacement")
hist_train_vs_test(auto_train$horsepower, auto_test$horsepower, "horsepower")
hist_train_vs_test(auto_train$weight, auto_test$weight, "weight")
hist_train_vs_test(auto_train$acceleration, auto_test$acceleration, "acceleration")
hist_train_vs_test(auto_train$origin, auto_test$origin, "origin")
hist_train_vs_test(auto_train$mpg01, auto_test$mpg01, "mpg01")
```

Although the shape is not exactly the same because it is drawn at random,
there was never a case where there were no or extremely fewer samples in a particular category.
So, Let's move on!


**(f) Perform logistic regression on the training data in order to predict mpg01 using the variables**
**that seemed most associated with mpg01 in (b). What is the test error of the model obtained?**

At the problem 3(b), I picked the 'weight' as the predictor.
So, my model becomes,

\[logit(P(y_i = 1 | x_{i}) = 
  \beta_0 + \beta_1 x_{i}\]
for i = 1,2,...,352, with 

- $y_i$: (binary factor) mpg01 of i-th observation
- $x_{i}$: weight of the i-th observation

and $\beta_1$ is the effect of weight. I will offer the exact interpretation of $\beta_1$ after fitting.

Here is the fitting result.

```{r}
auto_glm_fit <- glm(mpg01 ~ weight, data = auto_train)
summary(auto_glm_fit)
```

To test the model significance, I calculate the difference between the two deviances.

H0: $\beta_1 = 0$

H1: $\beta_1 \neq 0$

```{r}
87.955 - 38.371
1 - pchisq(87.955 - 38.371, 1)
```

The test statistic is $\chi^2$ is $87.955 - 38.371 = 49.584$, 
which follows the chi-square distribution with degrees of freedom 1.
The p-value is very close to 0, so we can reject H0 with typical significance levels like 0.05 or 0.01.
Thus, we can conclude that this model is significant.

The above model-significance test is also the test about the coefficient $\beta_1$.
So, we can say that $\beta_1$ is statistically not 0, or it is significant,
even if the estimated value of $\beta_1$ is very close to 0.
In this case, the odds value gets multiplied by $e^{0.0004493}$ as the weight increases by one unit.
Or, equivalently, the odds ratio is

\[\frac{P(Y=1|log{x}=c+1)/P(Y=0|log{x}=c+1)}{P(Y=1|log{x}=c)/P(Y=0|log{x}=c)} = e^{-0.0004493}\]

Here is a probability table at some selected 'weight' points.

```{r}
logit_inv <- function(z){
  return(exp(z) / (1 + exp(z)))
}

auto_coeff <- auto_glm_fit$coefficients
library(knitr)
kable(data.frame(
  weight = seq(1000, 5000, by = 500),
  probability = logit_inv(
    auto_coeff[1] + auto_coeff[2] * seq(1000, 5000, by = 500)
    )
  )
)
```

We can also draw the fitted curve on the scatterplot.

```{r}
par(mfrow = c(1, 1))
plot(mpg01 ~ weight, pch = 19, data = auto_train,
  xlim = c(1000, 5500), ylab = "mpg01", xlab = "weight")
curve(logit_inv(auto_coeff[1] + auto_coeff[2] * x), add = TRUE, col = "red")
```

The red line is the fitted curve.
Contrary to what I expected in the EDA step, the curve is quite gentle.
This is because the absolute value of the fitted coefficient is very close to 0.

Using the test dataset, let's evaluate this fitted model.
I will choose the cutoff point by a kind of grid-search (with the grid 0.00, 0.01, 0.02, ...,1.00).
Specifically, after calculating each cutoff value how much the model predicts correctly,
I will use the value that gave us the most correct prediction.

```{r}

auto_glm_fit_prob <- predict(auto_glm_fit, auto_test, type = "response")

cutoff_candidates <- seq(0, 1, by = 0.01)
best_score <- 0
best_cutoff <- 0
sens_ratio <- rep(0, length(cutoff_candidates))
falsepos_ratio <- rep(0, length(cutoff_candidates))

for (i in seq_len(length(cutoff_candidates))) {
  cutoff_val <- cutoff_candidates[i]
  auto_glm_pred_binary <- ifelse(auto_glm_fit_prob > cutoff_val, 1, 0)
  score <- sum(auto_glm_pred_binary == auto_test$mpg01)
  if (score > best_score) {
    best_cutoff <- cutoff_val
    best_score <- score
  }
  sens_ratio[i] <- sum(auto_test$mpg01 == 1 & auto_glm_pred_binary == 1) / sum(auto_test$mpg01 == 1)
  falsepos_ratio[i] <- sum(auto_test$mpg01 == 0 & auto_glm_pred_binary == 1) / sum(auto_test$mpg01 == 0)
}
print(best_cutoff)
```

The best cutoff value is 0.62.
Under this cutoff value, I make the confusion table.

```{r}
auto_glm_pred_binary <- ifelse(auto_glm_fit_prob > best_cutoff, 1, 0)
auto_glm_confusion_table <- table(
    pred = auto_glm_pred_binary, true = auto_test$mpg01)
auto_glm_confusion_table
```

Among 40 test set data, the model predict 37 cases (92.5%) correctly.
Let's draw the ROC curve using the test set.

```{r}
plot(falsepos_ratio, sens_ratio,
  type = "l", main = "ROC curve",
  xlim = c(0, 1), ylim = c(0, 1))
```

It looks very good.

Finally, let's see residual plots for the fitted model (of training data).
Left plots are residual plots with 'weight' on the horizontal axis,
and right plots are with 'fitted probability' on the horizontal axis.

```{r, fig.height=3.5}
auto_pearson_res <- residuals(auto_glm_fit, type = "pearson")
auto_standardized_res <- rstandard(auto_glm_fit)
auto_student_res <- rstudent(auto_glm_fit)
auto_dev_res <- residuals(auto_glm_fit, type = "deviance")
auto_fitted_prob <- logit_inv(auto_coeff[1] + auto_coeff[2] * auto_train$weight)

res_plot_func <- function(res, main){
  par(mfrow = c(1, 2))
  plot(auto_train$weight, res, main = main)
  abline(h = 0)
  plot(auto_fitted_prob, res, main = "")
  abline(h = 0)
}
res_plot_func(auto_pearson_res, main = "pearson residual")
res_plot_func(auto_standardized_res, main = "standardized residual")
res_plot_func(auto_student_res, main = "jacknife residual")
res_plot_func(auto_dev_res, main = "deviance residual")
```

We can see two lines for each residual plot.
In logistic regression, this pattern seems natural.
