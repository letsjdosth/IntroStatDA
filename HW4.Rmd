---
title: "Homework Assignment 4"
author: "Seokjun Choi"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

### \noindent Problem1.
**Albert and Rizzo, Chapter 9, problem 1**

**9.1 (Rounding first base). The data in “rounding.txt” gives the times**
**required to round first base for 22 baseball players using three styles: rounding**
**out, a narrow angle and a wide angle. The goal is to determine if the method**
**of rounding first base has a significant effect on times to round first base.**
**The data and the format of the data can be viewed using a text editor or**
**a spreadsheet. With the data file in the current working directory, input the**
**data using**

```{r}
rounding <- read.table("RX-data/rounding.txt", header = TRUE)
head(rounding)
```

**Check using the str function that the data is in stacked format with three**
**variables: time, method, player, where time is numeric and method and**
**player are factors.**

```{r}
str(rounding)
rounding$method <- factor(rounding$method)
rounding$block <- factor(rounding$block)
str(rounding)

par(mfrow = c(1, 1))
boxplot(times ~ method, data = rounding) #EDA
```

**Analyze the data assuming a randomized block design with time to round**
**first base as the response, method of rounding first as the treatment, and**
**player as the block variable. Plot residuals to check whether the assumptions**
**for the distribution of random error appear to be valid.**

Let $i=1,2,3$ be indices for methods and $j=1,2,...,22$ be indices for blocks (or, player).
Note that there is only one data point for each $(i,j)$. So I omit the cell-wise(?) index.
Then, the model becomes
\[y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}\]
where $\alpha_i$ are the effects of 'method', and $\beta_j$ are the block effects.
and $\epsilon_{ij}$ are iid, normally distributed errors with mean 0 and common variance $\sigma^2$.

```{r}
rounding_aov_fit <- aov(times ~ method + block, data = rounding)
summary(rounding_aov_fit)
```

Let's test whether the effects of 'method' and effects of blocks are significant or not.
hypotheses are

H0: all $\alpha_i$ are 0. (the method-effects are not significant on average.)

H1: at least one $\alpha_i$ is not 0. (the method-effects are significant on average.)

and

H0: all $\beta_i$ are 0. (the block effects are not significant.)

H1: at least one $\beta_i$ is not 0. (the block effects are significant.)

The anova table shows F values and p-values for each test.
For the former one, F value is 6.288, and it follows the F-distirbution with degrees of freedom 2 and 42 under H0.
The p-value is 0.00408, so se can reject the H0 with common significance levels like 0.01 or 0.05.
Thus, we can say that the method-effects are significant.

For the latter test, we can find that the F value is 26.970 which is pretty high, and under H0 it follows the
F-distribution with df 21 and 42. The p-value is very close to 0, so we can reject the H0 with ordinary significance level.
Therefore, we can say that the block effects are also significant.

Next, let's see the estimated values for the effects of 'methods'.
(We do not have an interest in interpreting the block effect.)
The upper table shows the estimated difference between means of each method and grand mean.
The lower table is about the mean values themselves.

```{r}
model.tables(rounding_aov_fit, cterms = "method")
model.tables(rounding_aov_fit, cterms = "method", type = "mean")
```

To see the aspect of the difference, let's use the tukey's significant difference method.

```{r}
rounding_tukeydiff <- TukeyHSD(rounding_aov_fit, which = 1) #only 'method'
rounding_tukeydiff
par(mfrow = c(1, 1))
plot(rounding_tukeydiff)
```

'WidAngle' is different significantly from other two methods.

Finally, for model diagnosis, let's draw the residual plot and the normal q-q plot.

```{r}
par(mfrow = c(2, 2))
plot(rounding_aov_fit)
```

In the residual plot, we can find a point that has high-residual value (7-th).
Not only that, the residuals have shape like a diamond. 
So we have to check whether the homogeneous variance assumption is valid or not.

In addition, in the normal q-q plot, tail parts of both two direction are not on a line.
It is questionable whether that the normal assumption is satisfied.
Especially, the 7-th data point is problematic. We need to act on that point.



### \noindent Problem2.
**Albert and Rizzo, Chapter 9, problem 2**

**9.2 (Speed of light). The morley data in R contains the classical data**
**of Michaelson and Morley on the speed of light, recording five experiments**
**of 20 consecutive runs each. The response is the speed of light measurement Speed.**
**The experiment is Expt and the run is Run. See the documentation (?morley)**
**and also http://lib.stat.cmu.edu/DASL/Stories/SpeedofLight.html**
**for more details about the experiments and the dataset.**

```{r}
head(morley)
# ?morley
```

According to the R help file, three variables are
- Expt: The experiment number, from 1 to 5.
- Run: The run number within each experiment.
- Speed: Speed-of-light measurement.

**Use the str function to check that there are 100 observations of the response**
**Speed, Expt, and Run; all integer variables.**

```{r}
str(morley)
```

**Convert Expt and Run to factors using**

```{r}
morley$Expt <- factor(morley$Expt)
morley$Run <- factor(morley$Run)
str(morley)
```


**Display a boxplot of Speed by Expt.**
**Speed of light is a constant, so we see there are some problems**
**because the measurements of speed do not seem to be consistent across the five experiments.**

```{r}
boxplot(Speed ~ Expt, data = morley)
```

**The data can be viewed as a randomized block experiment.**
**What is the null hypothesis of interest?**
**Analyze the data and residuals and summarize your conclusions.**

Let's test whether the measurements of light speed are consistent or not across the five experiments.
Because there is another factor, 'run', I will use it as a block.

Let $i=1,...,5$ be indices for the experiments and $j=1,2,...,20$ be indices for the run numbers.
Again, these dataset has one data point for each $(i,j)$. So it's not needed to use a subscript within each cell.

Then, the model becomes
\[y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}\]
where $\alpha_i$ are the experiment effects, and $\beta_j$ are the block (run) effects.
and $\epsilon_{ij}$ are iid, normally distributed errors with mean 0 and common variance $\sigma^2$.

Next, our hypothesis are
H0 : $\alpha_i=0$ for all $i$
(the measurements of light speed are consistent across the five experiments)

H1 : at least $\alpha_i$ is not 0.
(the measurements of light speed are not consistent across the five experiments)

```{r}
morley_aov_fit <- aov(Speed ~ Expt + Run, data = morley)
summary(morley_aov_fit)
```

The block effects are not significant. So, let's delete the variable. Then,
```{r}
morley_aov_fit2 <- aov(Speed ~ Expt, data = morley)
summary(morley_aov_fit2)
```

The F-value is 4.288, and it follows the F distribution with df 4 and 95.
The p-value is 0.00311, so we can reject H0 under significance level higher than 0.00311, like 0.01 or 0.05.
Thus, we cannot say that the measurements of light speed are not consistent across the five experiments.

Here are differences from grand mean, and mean values of the experiment groups.
```{r}
model.tables(morley_aov_fit2, cterms = "Expt")
model.tables(morley_aov_fit2, cterms = "Expt", type = "mean")
```

And, tukey's difference test says that there are differences between experiment 1 and 4, and between 1 and 5.
```{r}
morley_tukeydiff <- TukeyHSD(morley_aov_fit2, which = 1)
morley_tukeydiff
par(mfrow = c(1, 1))
plot(morley_tukeydiff)
```

Finally, here are the residual plot and the normal q-q plot.

```{r}
par(mfrow = c(2, 2))
plot(morley_aov_fit2)
```

Except for two or three points that seem outliers, they looks fine.
We cannot find any pattern from the residual plot, 
and almost all points are making a line in the q-q plot (if we delete the three points).


### \noindent Problem3.
**The data frame tb.dilute is available in the ISwR package.**
**Perform a two-way analysis of variance of these data.**
**Explain your model, analysis and conclusions.**

```{r}
library(ISwR)
tb <- tb.dilute
# ?tb.dilute
head(tb)
is.factor(tb$animal)
is.factor(tb$logdose)
```
According to the tb.dilute's R help file, 
this dataset consists of 18 data points, with three variables.
Each variable is
- reaction : a numeric vector, reaction sizes (average of diameters) for tuberculin skin pricks.
- animal: a factor with levels 1–6.
- logdose: a factor with levels 0.5, 0, and -0.5.

```{r}
par(mfrow = c(1, 2))
boxplot(reaction ~ animal, data = tb) #EDA
boxplot(reaction ~ logdose, data = tb)
```

we can find that the reaction sizes increase as the animal factor's values increase, and
the reaction sizes decrease as the logdose factor's values decrease.


When considering the type of each variable, a natural model is

\[y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}\]
for $i=1,2,...6, j=1,2,3$,
where $y_{ij}$ are reaction sizes of i-th animal factor and j-th logdose factor,
and $\alpha_i$ are i-th animal factor effect, $\beta_j$ are j-th logdose factor effect
(0.5, 0, -0.5, respectively),
and $\epsilon_{ij}$ are iid error, following $N(0, \sigma^2)$.

Note that we cannot add an interaction term, because there is only one data point
for each (i,j)-combination. If we add the term, the degrees of freedom of residual would be 0.

Let's fit the model.
```{r}
tb_aov_fit <- aov(reaction ~ animal + logdose, data = tb)
summary(tb_aov_fit)
```

To check whether each effect is significant or not, we can do tests.
Specifically, for $\alpha_i$,

H0: all $\alpha_i$ are 0. (There is no effect of animal factor on average.)

H1: there is at least different $\alpha_i$. (The effect of animal factor is significant on average.)

and for $\beta_j$,

H0: all $\beta_j$ are 0. (There is no effect of logdose factor on average.)

H1: there is at least different $\beta_j$. (The effect of logdose factor is significant on average.)

Let's see the above anova table that R gives to us
The F-value of the first test is 8.264, which following the F-distribution with df 5 and 10 under H0.
The p-value is 0.00253, so we can reject the H0 with significance level larger than 0.00253, like 0.05 or 0.01.
Thus, we can say that the effect of animal factor is significant.

Next, in case of the $\beta_j$'s test,
the F-value is 42.482, which following the F-distribution with df 2 and 10 under H0.
The p-value is close to 0, so we can reject the H0 with a common significance level.
Therefore, we can conclude that the effect of logdose factor is also significant.


Now, using fitted values and Tukey’s multiple comparison method, let's compare group means.

```{r}
model.tables(tb_aov_fit, type = "means")
tb_tukeydiff <- TukeyHSD(tb_aov_fit, which = c("animal", "logdose"))
tb_tukeydiff
par(mfrow = c(1, 2))
plot(tb_tukeydiff, las = 1)
```

Three pairs of animal factors (5-1, 6-1, 6-2) are different.
Not only that, two pairs of logdose factor (between -0.5 and 0.5, and between -0.5 and 0)
does not include 0 in their CI.

Finally, here are the residual plot and the normal q-q plot.
```{r}
par(mfrow = c(2, 2))
plot(tb_aov_fit)
```

In the both plots, we can find two outliers (13th and 14th points).
Even if we talk except for those two points, there are other problems.
We can find that a weak U-shape pattern in the residual plot.
And we can see that data points are not perfectly on the line in the normal q-q plot,

But depending on point of view, some other researcher may not think
that it is terribly problematic except for two outliers.
The U-shape is very weak, and the data points are 'nearly' linear in the q-q plot.
It is also known that ANOVA is quite robust!
So, I guess someone might say it's okay. It's a bit vague situation.




### \noindent Problem4.
**Analyze the vitcap2 dataset in the ISwR package using analysis of covariance.**
**Explain your model, analysis and conclusions.**

```{r}
library(ISwR)
# ?vitcap2
vital <- vitcap2
vital$group <- factor(vital$group)
str(vital)
```

'vitcap2' dataset has 84 data points and 3 variables. Each column is

- group : a numeric vector; group codes are 1: exposed > 10 years, 2: exposed < 10 years, 3: not exposed.
- age : a numeric vector, age in years.
- vital.capacity : a numeric vector, vital capacity (a measure of lung volume) (l).

Note that each group has different number of points. In other words, this dataset is imbalanced.
```{r}
dim(subset(vital, group == 1))
dim(subset(vital, group == 2))
dim(subset(vital, group == 3))
```


Let's do EDA seeing some figures.
```{r}
par(mfrow = c(1, 2))
boxplot(vital.capacity ~ group, data = vital)
colors <- c("#66BD63", "red", "blue")
plot(vital.capacity ~ age, data = vital, pch = 19, col = colors[group])
legend("bottomleft", pch = 19, col = colors,
  legend = c("group 1", "group 2", "group 3"))
```

In the boxplot, we can find that group 2's and 3's medians of vital capacity are similar, 
but seem different from group 1's.
Not only that, in the scatterplot,
we can see the negative relationship between age and vital capacity.

Let's consider a model,
\[y_{ij} = \mu +\alpha_i + (\beta + \gamma_i) x_{ij} + \epsilon\]
for $i=1,2,3$, $j=1,...,n_i$, $n_1=12, n_2=28, n_3=44$
where
- $y_{ij}$ : vital capacity of i-th group, j-th subject
- $\alpha_i$ : i-th group effect
- $x_{ij}$ : age of i-th group, j-th subject
- $\gamma_{ij}$ : interaction with age and group
and assume that $\epsilon ~ N(0, \sigma^2)$, iid.

Here is the fitting result.
```{r}
vital_lm_fit <- lm(vital.capacity ~ group * age, data = vital)
summary(vital_lm_fit)
anova(vital_lm_fit)
```

To begin with, let's do the F test to show whether this model is significant or not.

H0 : all $\alpha_i$, $\beta$, $\gamma_i$ are 0.

H1 : at least one $\alpha_i$, $\beta$, $\gamma_i$ is not 0.

The F-statistic value is 11.39, which following the F distribution with df 5 and 78 under H0.
The p-value is very close to 0, so we can reject H0. Thus this model is meaningful.

Next, let's see the anova table.
First hypotheses are

H0 : all $\gamma_i$ are 0.

H1 : at least one $\gamma_i$ is not 0.

The F value of 'group:age' is 3.5402, which follows the F-distribution with df 2, 78 under H0.
If we use a significance level 0.05, we cannot reject H0. But using 0.01, we can.

Next,

H0 : all $\alpha_i$ are 0.

H1 : at least one $\alpha_i$ is not 0.

The F value of 'group' is 3.8912, which follows the F-distribution with df 2, 78 under H0.
Again, if we use a significance level 0.05, we cannot reject the H0. But using 0.01, we can reject it.

Finally,

H0 : $\beta = 0$

H1 : $\beta \neq 0$

The F-value of age is 42.0915, which follows the F-distribution with df 1, 78 under H0.
The p-value is nearly 0, so we can reject H0.


So, if we use 0.01 significance level, we can reduce the model to simpler one like
\[y_{j} = \mu + \beta x_{ij} + \epsilon\]
for $j=1,...,84$. But I'll skip this work.


To sum up the fitting coefficients, for group 1,
\[\hat{y_{1j}} = 8.1834 - 0.0851 x_{1j}\]
and for group 2,
\[\hat{y_{2j}} = 8.1834 - 1.9534 + (-0.0851 + 0.0386) x_{2j}\]
and for group 3,
\[\hat{y_{3j}} = 8.1834 - 2.5031 + (-0.0851 + 0.0545) x_{3j}\]

```{r}
par(mfrow = c(1, 1))
plot(vital.capacity ~ age, data = vital, pch = 19, col = colors[group])
legend("bottomleft", pch = 19, col = colors,
  legend = c("group 1", "group 2", "group 3"))
vital_coeff <- vital_lm_fit$coefficients
vital_coeff
abline(vital_coeff[1], vital_coeff[4], col = colors[1])
abline(vital_coeff[1]+vital_coeff[2], vital_coeff[4]+vital_coeff[5], col = colors[2])
abline(vital_coeff[1]+vital_coeff[3], vital_coeff[4]+vital_coeff[6], col = colors[3])
```

Finally, here are the residual plots and the normal q-q plot.

```{r}
par(mfrow = c(2, 2))
plot(vital_lm_fit)
```

The residual plot seems fine.
In the normal q-q plot, we can find that points on the right tail part are not on the straight line.
This appears to violate the normal distribution assumption weakly.



### \noindent Problem5.
**An experiment was run to investigate the amount of weight loss (in grams)**
**by ground beef hamburgers after grilling or frying, and**
**how much the weight loss is affected by the percentage of fat in the beef before cooking.**
**The experiment involved 2 factors:**

- cooking method (with two levels, frying and grilling)
- fat content (with 3 levels: 10%, 15% and 20%)

**Hamburger patties weighing 110g each were prepared from meat with the required fat content.**
**There were 30 cooking time slots which were randomly assigned to the treatments**
**in such a way that each treatment was observed 5 times.**
**The patty weighs after cooking are shown below:**

- Method Frying, Fat Content 10%: 81, 88, 85, 84, 84
- Method Frying, Fat Content 15%: 85, 80, 82, 80, 82
- Method Frying, Fat Content 20%: 71, 77, 72, 80, 80
- Method Grilling, Fat Content 10%: 84, 84, 82, 81, 86
- Method Grilling, Fat Content 15%: 83, 88, 85, 86, 88
- Method Grilling, Fat Content 20%: 78, 75, 78, 79, 82 

```{r}
method <- factor(rep(c("frying", "grilling"), each = 5 * 3))
fat <- factor(rep(c("10%", "15%", "20%"), each = 5, times = 2))
weightloss <- c(81, 88, 85, 84, 84, 85, 80, 82, 80, 82, 71, 77, 72, 80, 80,
  84, 84, 82, 81, 86, 83, 88, 85, 86, 88, 78, 75, 78, 79, 82)
wl_data <- data.frame(method, fat, weightloss)
```

**1. Perform EDA.**

To begin with, let's draw boxplots.

```{r}
par(mfrow = c(1, 2))
boxplot(weightloss ~ method, data = wl_data)
boxplot(weightloss ~ fat, data = wl_data)
par(mfrow = c(1, 1))
boxplot(weightloss ~ method + fat, data = wl_data,
  las = 2, cex.axis = 0.8, xlab = "")
```

The difference between two methods seems small.
However, in the case of the fat, the level '20%' is very different from the other two levels.
The box of 20% even does not overlap the other two boxes.

```{r}
par(mfrow = c(1, 2))
with(data = wl_data, expr = {
interaction.plot(method, fat, response = weightloss)
interaction.plot(fat, method, response = weightloss)
})
```

In interaction plots, each lines are not parallel.
It implies there may be an interaction between 'method' and 'fat'.


**2. Perform a two-way analysis of variance.**

**Explain your model and your conclusions. In particular:**

**- Compare the effects of the 3 levels of fat (taking into account cooking methods if needed) and interpret your results**

**- Give a 90% confidence interval for the mean difference in weight after cooking between frying and grilling 110g hamburgers.**

Now, our model is
\[y_{ijk} = \mu + \alpha_i + \beta_j + \gamma_{ij} + \epsilon_{ijk}\]
for $i=1,2$, $j=1,2,3$, and $k=1,2,3,4,5$, 
where

- y_{ijk}: amount of weight loss of i-th method(frying, grilling), j-th fat level (10%, 15%, 20%), and k-th observations
- $\alpha_i$: i-th method effect
- $\beta_j$: j-th fat level effect
- $\gamma_{ij}$ : interaction effect between i-th method and j-th fat level

and $\epsilon_{ijk}$ are iid normal random variable with mean 0 and variance $\sigma^2$.

```{r}
wl_aov_fit <- aov(weightloss ~ method * fat, data = wl_data)
summary(wl_aov_fit)
```

First test is for the interaction term.

H0 : all $\gamma_{ij}$ are 0.

H1 : at least one $\gamma_{ij}$ is not 0.

The F-value is 2.399, following the F-distribution with df 2 and 24.
We cannot reject H0 under common significance levels like 0.05 or 0.01,
because the p-value (0.112) is larger than these significance levels.
So, we cannot conclude that the interaction term is statistically significant.

Thus, let's reduce the model to the simpler one.

Now, our model becomes

\[y_{ijk} = \mu + \alpha_i + \beta_j + \epsilon_{ijk}\]
for $i=1,2$, $j=1,2,3$, and $k=1,2,3,4,5$, 
under the same notation and assumption.

The fitting result for the new model is here.
```{r}
wl_aov_fit2 <- aov(weightloss ~ method + fat, data = wl_data)
summary(wl_aov_fit2)
```

First Hypotheses are

H0: All $\alpha_i$ are 0.

H1: At least one $\alpha_i$ is 0. (or, $\alpha_2 \neq 0$)

The F-value of the 'method' is 3.247, which follows the F-distribution with df 1 and 26.
The p-value is 0.0832, so we cannot reject H0 under common significance level like 0.05 or 0.01.
In other words, we cannot conclude that there is a difference between two cooking methods.

Second Hypotheses are

H0: All $\beta_j$ are 0.

H1: At least one $\beta_j$ is not 0.

The F-value of the 'fat' is 18.591, following the F-distribution with df 2 and 26.
The p-value is so close to 0, so we can reject the H0.
This means that there is a significant difference on the amount of weight loss among fat levels.


Next, next table show fitted mean differences from grand mean by each level of fat.

```{r}
model.tables(wl_aov_fit2, cterms = "fat")
```

And, this table show the mean values for each level of fat.

```{r}
model.tables(wl_aov_fit2, cterms = "fat", type = "mean")
```

To see how they are differ, I will use the tukey's significant difference method.
Following the problem's direction, I use 90% confidence level for this work.

```{r}
wl_tukeydiff <- TukeyHSD(wl_aov_fit2, which = 2, conf = 0.9) #for fat
wl_tukeydiff
par(mfrow = c(1, 1))
plot(wl_tukeydiff)
```

From this R output, we can see the 90% confidence interval for the mean difference in weight among three fat level.
Between the fat levels 10% and 20%, and between the levels 15% group and 20% group,
we can find significant differences on their average.

In addition, here are 90% CIs between two coocking methods, which are requested in the problem.

```{r}
wl_tukeydiff2 <- TukeyHSD(wl_aov_fit2, which = 1, conf = 0.9) #for method
wl_tukeydiff2
par(mfrow = c(1, 1))
plot(wl_tukeydiff2)
```

This 90% CI does not include zero.
This is a consistent result because the p-value of the F test about 'method' is 0.070.
Under significance level 0.1, we cannot reject H0.
(Note that the tests described in the previous paragraph used different significance levels, 0.05 or 0.01,
so I rejected H0.)

Finally, for model diagnosis, let's draw the residual plot and the normal q-q plot.

```{r}
par(mfrow = c(2, 2))
plot(wl_aov_fit2)
```

All things look fine. We cannot find an outlier nor a pattern in the residual plot.
Points on the normal q-q plot seem straight enough.


### \noindent Problem6.
**In the ISwR malaria dataset, analyze the risk of malaria via logistic regression**
**with age and the log-transformed antibody level as explanatory variables.**
**Explain your model, analysis and conclusions.**

```{r}
library(ISwR)
str(malaria)
# ?malaria
summary(malaria)
```
There are 100 data points, and 4 variables.
Each variable is

- subject: subject code.
- age: age in years.
- ab: antibody level.
- mal: a numeric vector code, Malaria: 0: no, 1: yes.


Let's look at the data.

```{r}
par(mfrow = c(2, 1))
plot(jitter(mal, 0.5) ~ jitter(age, 0.5), pch = 19, data = malaria,
  xlim = c(-1, 10), ylab = "malaria", xlab = "age")
plot(jitter(mal, 0.15) ~ log(ab), pch = 19, data = malaria, 
  xlim = c(-1, 10), ylab = "malaria", xlab = "log(antibody level)")
```
```{r}
par(mfrow = c(1, 1))
colors <- c("red", "blue")
plot(log(ab) ~ age, col = colors[mal + 1], pch = 19, data = malaria)
legend("bottomleft", pch = 19, col = colors,
  legend = c("mal-no", "mal-yes"))
```

It's hard to find a relation between age and malaria.
However, it seems that someone who has a higher log-transformed antibody level is less likely to have malaria.
Note that there is no explanatory variable of complete separation.


Let's set a model about the probability of having malaria using other two variables, 'age' and log transformed 'ab'.
Now, our model is
\[logit(P(y_i = 1 | x_{i1}, x_{i2})) = \beta_0 + \beta_1 x_{i1} + \beta_2 *x_{i2}\]
for i = 1,2,...,100, where

- $y_i$: 1 if i-th sample has malaria, 0 otherwise
- $x_i1$: age of i-th sample
- $x_i2$: log transformed antibody level of i-th sample

```{r}
mal_glm_fit <- glm(mal ~ age + log(ab), data = malaria, family = "binomial")
summary(mal_glm_fit)
```

To start with, let's check whether this model is significant or not.

H0 : $\beta_j$ ($j=1,2$) are 0.

H1 : at least one $\beta_j$ ($j=1,2$) is not 0.

```{r}
116.652 - 98.017 #chi-sq value
1 - pchisq(18.635, 2) #p-value
```

The difference of two deviances is 19.635, which is following chi-square distribution with df 2 under H0.
The p-value is very close to 0. We can reject H0 under common significance levels like 0.05 or 0.01.
Thus this model is statistically significant.

Second, let's see each coefficient.
Age is not significant, if we consider the z-test.
However, according to Agresti's book, a wald type test may be problematic 
for a logistic regression result.
But this case, the age variable may have little role 
when we consider the scattorplot in the above EDA procedure.
So, I think we can exclude the 'age' variable our model.


Now, new model is
Let's set a model about the probability of having malaria using other two variables, 'age' and log transformed 'ab'.
Now, our model is
\[logit(P(y_i = 1 | x_{i1}, x_{i2})) = \alpha_0 + \alpha_1 *x_{i}\]
for i = 1,2,...,100, where

- $y_i$: 1 if i-th sample has malaria, 0 otherwise
- $x_i$: log transformed antibody level of i-th sample

```{r}
mal_glm_fit2 <- glm(mal ~ log(ab), data = malaria, family = "binomial")
summary(mal_glm_fit2)
```

Using deviance values, let's test new model's significance.

H0 : $\alpha_1 = 0$

H1 : $\alpha_1 \neq 0$

```{r}
116.652 - 98.968 # chi-square value
1 - pchisq(17.684, 1) #p-value
```

The difference of two deviances is 17.684, which is following chi-square distribution with df 1 under H0.
The p-value is very close to 0. We can reject H0 under common significance levels like 0.05 or 0.01.
Thus this model is statistically significant, and $\alpha_1$ is not 0.

The estimated $\alpha_1$ is -0.7122.
If log-transformed antibody level increase 1 unit,
the odd is multiplied by $e^{-0.7122}$.
Or equivalently, the odd ratio is
\[\frac{P(Y=1|X=c+1)/P(Y=0|X=c+1)}{P(Y=1|X=c)/P(Y=0|X=c)} = e^{-0.7122}\]

For example,
```{r}
logit_inv <- function(z){
  return(exp(z) / (1 + exp(z)))
}
mal_coeff <- mal_glm_fit2$coefficients
library(knitr)
kable(data.frame(log_ab = 0:7, probability = logit_inv(mal_coeff[1] + mal_coeff[2] * (0:7))))
```

We can draw the fitted curve.
```{r}
par(mfrow = c(1, 1))
plot(jitter(mal, 0.15) ~ log(ab), pch = 19, data = malaria,
  xlim = c(-1, 10), ylab = "malaria", xlab="log(antibody level)")
curve(logit_inv(mal_coeff[1] + mal_coeff[2] * x), add = TRUE, col = "red")
```

Let's evaluate this fitting result.

```{r}
mal_pred_prob <- predict(mal_glm_fit2, data.frame(ab = malaria$ab), type = "response")
mal_pred_binary <- ifelse(mal_pred_prob > 0.5, 1, 0)
sum(malaria$mal == mal_pred_binary)
sum(malaria$mal == 1 & mal_pred_binary == 1) / sum(malaria$mal == 1) #sensitivity
sum(malaria$mal == 0 & mal_pred_binary == 0) / sum(malaria$mal == 0) #specificity
```
If we choose cutoff point as 0.5, the 72% points are correctly predicted.
But sensitivity $P(\hat{y}=1 | y=1)$ is low.

```{r}
par(mfrow = c(1, 1))
plot(0, 0, type = "n", xlim = c(0, 1), ylim = c(0, 1),
  xlab = "false positive", ylab = "sensitivity", main="ROC curve")
abline(0, 1, lty=2)

log_ab_df <- data.frame(ab = malaria$ab)

for(thres in seq(0, 1, by = 0.005)) {
  mal_pred_prob <- predict(mal_glm_fit2, log_ab_df, type = "response")
  mal_pred_binary <- ifelse(mal_pred_prob > thres, 1, 0)
  #data points of malaria data == 100
  sens_ratio <- sum(malaria$mal == 1 & mal_pred_binary == 1) / sum(malaria$mal == 1)
  falsepos_ratio <- sum(malaria$mal == 0 & mal_pred_binary == 1) / sum(malaria$mal == 0)

  points(falsepos_ratio, sens_ratio, pch = 19)
}
```

The ROC curve seems good.
Note that I draw this plot by re-using the training dataset.
I do not seperate a training dataset and a testing dataset.
This may be a limitation in evaluating predictive powerof this model
by looking at the above ROC curve.
However, for now, I skip the cross-validation procedure.

Finally, I attach the diagnostic plots without explanation.
(Residual analysis for logistic regression models hasn't been covered in class yet.)

```{r}
par(mfrow = c(2, 2))
plot(mal_glm_fit2)
```