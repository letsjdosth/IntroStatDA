---
title: "Homework Assignment 4"
author: "Seokjun Choi"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

### \noindent Problem1.
**Albert and Rizzo, Chapter 9, problem 1**

**9.1 (Rounding first base). The data in “rounding.txt” gives the times**
**required to round first base for 22 baseball players using three styles: rounding**
**out, a narrow angle and a wide angle. The goal is to determine if the method**
**of rounding first base has a significant effect on times to round first base.**
**The data and the format of the data can be viewed using a text editor or**
**a spreadsheet. With the data file in the current working directory, input the**
**data using**

```{r}
rounding <- read.table("RX-data/rounding.txt", header = TRUE)
head(rounding)
```

**Check using the str function that the data is in stacked format with three**
**variables: time, method, player, where time is numeric and method and**
**player are factors.**

```{r}
str(rounding)
rounding$method <- factor(rounding$method)
rounding$block <- factor(rounding$block)
str(rounding)

par(mfrow = c(1, 1))
boxplot(times ~ method, data = rounding) #EDA
```

Several points are far from the box.
We may have to be careful that the analysis result is distorted by these points.


**Analyze the data assuming a randomized block design with time to round**
**first base as the response, method of rounding first as the treatment, and**
**player as the block variable. Plot residuals to check whether the assumptions**
**for the distribution of random error appear to be valid.**

Let $i=1,2,3$ be indices for methods and $j=1,2,...,22$ be indices for blocks (or, player).
Note that there is only one data point for each $(i,j)$. So I omit the index within each cell.
Then, the model becomes
\[y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}\]
where $\alpha_i$ are the effects of 'method', and $\beta_j$ are the block effects.
and $\epsilon_{ij}$ are iid, normally distributed errors with mean 0 and common variance $\sigma^2$.

```{r}
rounding_aov_fit <- aov(times ~ method + block, data = rounding)
summary(rounding_aov_fit)
```

Let's test whether the effects of 'method' and effects of blocks are significant or not.
hypotheses are

H0: all $\alpha_i$ are 0. (the method-effects are not significant on average.)

H1: at least one $\alpha_i$ is not 0. (the method-effects are significant on average.)

and

H0: all $\beta_i$ are 0. (the block effects are not significant.)

H1: at least one $\beta_i$ is not 0. (the block effects are significant.)

The anova table shows F values and p-values for each test.

For the block effect test, we can find that the F value is 26.970 which is pretty high, and under H0 it follows the
F-distribution with df 21 and 42. The p-value is very close to 0, so we can reject the H0 with an ordinary significance level.
Therefore, we can say that the block effects are also significant. So this block term cannot be removed.

Next, for the former one, F value is 6.288, and it follows the F-distribution with degrees of freedom 2 and 42 under H0.
The p-value is 0.00408, so we can reject the H0 with common significance levels like 0.01 or 0.05.
Thus, we can say that the method-effects are significant.

Next, let's see the estimated values for the effects of 'methods'.
(We do not have an interest in interpreting the block effect.)
The upper table shows the estimated difference between the mean of each method and the grand mean.
The lower table is about the mean values themselves.

```{r}
model.tables(rounding_aov_fit, cterms = "method")
model.tables(rounding_aov_fit, cterms = "method", type = "mean")
```

To see the aspect of the difference, let's use Tukey's significant difference method.

```{r}
rounding_tukeydiff <- TukeyHSD(rounding_aov_fit, which = 1) #only 'method'
rounding_tukeydiff
par(mfrow = c(1, 1))
plot(rounding_tukeydiff)
```

'WidAngle' is different significantly from the other two methods.

Finally, for model diagnosis, let's draw the residual plot and the normal q-q plot.

```{r}
par(mfrow = c(2, 2))
plot(rounding_aov_fit)
```

In the residual plot, we can find a point that has a high-residual value (7th).
Not only that, the residuals have a shape like a diamond. 
So we have to check whether the homogeneous variance assumption is valid or not.

In addition, in the normal q-q plot, tail parts of both two directions are not on a line.
It is questionable whether that the normal assumption is satisfied.
Especially, the 7th data point is problematic. We need to take action on that.
(For example, we may discard the point.)


### \noindent Problem2.
**Albert and Rizzo, Chapter 9, problem 2**

**9.2 (Speed of light). The morley data in R contains the classical data**
**of Michaelson and Morley on the speed of light, recording five experiments**
**of 20 consecutive runs each. The response is the speed of light measurement Speed.**
**The experiment is Expt and the run is Run. See the documentation (?morley)**
**and also http://lib.stat.cmu.edu/DASL/Stories/SpeedofLight.html**
**for more details about the experiments and the dataset.**

```{r}
head(morley)
# ?morley
```

According to the R help file, three variables are

- Expt: The experiment number, from 1 to 5.
- Run: The run number within each experiment.
- Speed: Speed-of-light measurement.

**Use the str function to check that there are 100 observations of the response**
**Speed, Expt, and Run; all integer variables.**

```{r}
str(morley)
```

**Convert Expt and Run to factors using**

```{r}
morley$Expt <- factor(morley$Expt)
morley$Run <- factor(morley$Run)
str(morley)
```


**Display a boxplot of Speed by Expt.**
**Speed of light is a constant, so we see there are some problems**
**because the measurements of speed do not seem to be consistent across the five experiments.**

```{r}
boxplot(Speed ~ Expt, data = morley)
```

**The data can be viewed as a randomized block experiment.**
**What is the null hypothesis of interest?**
**Analyze the data and residuals and summarize your conclusions.**

Let's test whether the measurements of light speed are consistent or not across the five experiments.
Because there is another factor, 'run', I will use it as a block.

Let $i=1,...,5$ be indices for the experiments and $j=1,2,...,20$ be indices for the run numbers.
This dataset has one data point for each $(i,j)$. So it's not needed to use a subscript within each cell.

Then, the model becomes
\[y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}\]
where $\alpha_i$ are the experiment effects, and $\beta_j$ are the block (run) effects.
and $\epsilon_{ij}$ are iid, normally distributed errors with mean 0 and common variance $\sigma^2$.

Next, our hypotheses are

H0 : $\alpha_i=0$ for all $i$
(the measurements of light speed are consistent across the five experiments)

H1 : at least $\alpha_i$ is not 0.
(the measurements of light speed are not consistent across the five experiments)

```{r}
morley_aov_fit <- aov(Speed ~ Expt + Run, data = morley)
summary(morley_aov_fit)
```

The block effects are not significant. So, let's delete the variable. Then,

```{r}
morley_aov_fit2 <- aov(Speed ~ Expt, data = morley)
summary(morley_aov_fit2)
```

The F-value is 4.288, and it follows the F distribution with df 4 and 95.
The p-value is 0.00311, so we can reject H0 under a significance level higher than 0.00311, like 0.01 or 0.05.
Thus, we cannot say that the measurements of light speed are not consistent across the five experiments.

Here are differences from grand mean, and mean values of the experiment groups.

```{r}
model.tables(morley_aov_fit2, cterms = "Expt")
model.tables(morley_aov_fit2, cterms = "Expt", type = "mean")
```

And, Tukey's difference test says that there are differences between experiments 1 and 4, and between 1 and 5.

```{r}
morley_tukeydiff <- TukeyHSD(morley_aov_fit2, which = 1)
morley_tukeydiff
par(mfrow = c(1, 1))
plot(morley_tukeydiff)
```

Finally, here are the residual plot and the normal q-q plot.

```{r}
par(mfrow = c(2, 2))
plot(morley_aov_fit2)
```

Except for two or three points that seem outliers, they look fine.
We cannot find any pattern from the residual plot, 
and almost all points are making a line in the q-q plot (if we delete the three points).


### \noindent Problem3.
**The data frame tb.dilute is available in the ISwR package.**
**Perform a two-way analysis of variance of these data.**
**Explain your model, analysis and conclusions.**

```{r}
library(ISwR)
tb <- tb.dilute
# ?tb.dilute
head(tb)
is.factor(tb$animal)
is.factor(tb$logdose)
```

According to the tb.dilute's R help file, 
this dataset consists of 18 data points, with three variables.
Each variable is

- reaction: a numeric vector, reaction sizes (average of diameters) for tuberculin skin pricks.
- animal: a factor with levels 1–6.
- logdose: a factor with levels 0.5, 0, and -0.5.

```{r}
par(mfrow = c(1, 2))
boxplot(reaction ~ animal, data = tb) #EDA
boxplot(reaction ~ logdose, data = tb)
```

we can find that the reaction sizes increase as the animal factor's values increase, and
the reaction sizes decrease as the logdose factor's values decrease.


When considering the type of each variable, a natural model is

\[y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}\]
for $i=1,2,...6, j=1,2,3$,
where $y_{ij}$ are reaction sizes of i-th animal factor and j-th logdose factor,
and $\alpha_i$ are i-th animal factor effect, $\beta_j$ are j-th logdose factor effect
(0.5, 0, -0.5, respectively),
and $\epsilon_{ij}$ are iid error, following $N(0, \sigma^2)$.

Note that we cannot add an interaction term, because there is only one data point
for each (i,j)-combination. If we add the term, the degrees of freedom of residual would be 0.

Let's fit the model.

```{r}
tb_aov_fit <- aov(reaction ~ animal + logdose, data = tb)
summary(tb_aov_fit)
```

To check whether each effect is significant or not, we can do tests.
Specifically, for $\alpha_i$,

H0: all $\alpha_i$ are 0. (There is no effect of the animal factor on average.)

H1: there is at least different $\alpha_i$. (The effect of the animal factor is significant on average.)

and for $\beta_j$,

H0: all $\beta_j$ are 0. (There is no effect of the logdose factor on average.)

H1: there is at least different $\beta_j$. (The effect of the logdose factor is significant on average.)

Let's see the above anova table that R gives to us
The F-value of the first test is 8.264, which follows the F-distribution with df 5 and 10 under H0.
The p-value is 0.00253, so we can reject the H0 with a significance level larger than 0.00253, like 0.05 or 0.01.
Thus, we can say that the effect of the animal factor is significant.

Next, in the case of the $\beta_j$'s test,
the F-value is 42.482, which follows the F-distribution with df 2 and 10 under H0.
The p-value is close to 0, so we can reject the H0 with a common significance level.
Therefore, we can conclude that the effect of the logdose factor is also significant.


Now, using fitted values and Tukey’s multiple comparison method, let's compare group means.

```{r}
model.tables(tb_aov_fit, type = "means")
tb_tukeydiff <- TukeyHSD(tb_aov_fit, which = c("animal", "logdose"))
tb_tukeydiff
par(mfrow = c(1, 2))
plot(tb_tukeydiff, las = 1)
```

Three pairs of the animal factor (5-1, 6-1, 6-2) are different.
Not only that, two pairs of the logdose factor (between -0.5 and 0.5, and between -0.5 and 0)
does not include 0 in their CI.

Finally, here are the residual plot and the normal q-q plot.

```{r}
par(mfrow = c(2, 2))
plot(tb_aov_fit)
```

In the both plots, we can find two outliers (13th and 14th points).
Even if we talk except for those two points, there are other problems.
We can find that a weak U-shape pattern in the residual plot.
And we can see that data points are not perfectly on the line in the normal q-q plot.

However, depending on point of view, some other researchers may not think
that it is terribly problematic except for two outliers.
The U-shape is very weak, and the data points are 'nearly' linear in the q-q plot.
It is also known that ANOVA is robust.
So, I guess someone might say it's okay. It's a bit vague situation.




### \noindent Problem4.
**Analyze the vitcap2 dataset in the ISwR package using analysis of covariance.**
**Explain your model, analysis and conclusions.**

```{r}
library(ISwR)
# ?vitcap2
vital <- vitcap2
vital$group <- factor(vital$group)
str(vital)
```

'vitcap2' dataset has 84 data points and 3 variables. Each column is

- group: a numeric vector; group codes are 1: exposed > 10 years, 2: exposed < 10 years, 3: not exposed.
- age: a numeric vector, age in years.
- vital.capacity: a numeric vector, vital capacity (a measure of lung volume) (l).

Note that each group has a different number of points. In other words, this dataset is imbalanced.

```{r}
dim(subset(vital, group == 1))
dim(subset(vital, group == 2))
dim(subset(vital, group == 3))
```


Let's do EDA seeing some figures.

```{r}
par(mfrow = c(1, 2))
boxplot(vital.capacity ~ group, data = vital)
colors <- c("#66BD63", "red", "blue")
plot(vital.capacity ~ age, data = vital, pch = 19, col = colors[group])
legend("bottomleft", pch = 19, col = colors,
  legend = c("group 1", "group 2", "group 3"))
```

In the boxplot, we can find that group 2's and 3's medians of vital capacity are similar, 
but seem different from group 1's.
Not only that, in the scatterplot,
we can see the negative relationship between age and vital capacity.

Let's consider a model,
\[y_{ij} = \mu +\alpha_i + (\beta + \gamma_i) x_{ij} + \epsilon\]
for $i=1,2,3$, $j=1,...,n_i$, $n_1=12, n_2=28, n_3=44$
where

- $y_{ij}$ : vital capacity of i-th group, j-th subject
- $\alpha_i$ : i-th group effect
- $x_{ij}$ : age of i-th group, j-th subject
- $\gamma_{ij}$ : interaction with age and group
and assume that $\epsilon ~ N(0, \sigma^2)$, iid.

Here is the fitting result.

```{r}
vital_lm_fit <- lm(vital.capacity ~ group * age, data = vital)
summary(vital_lm_fit)
anova(vital_lm_fit)
```

To begin with, let's do the F test to show whether this model is significant or not.

H0 : all $\alpha_i$, $\beta$, $\gamma_i$ are 0.

H1 : at least one $\alpha_i$, $\beta$, $\gamma_i$ is not 0.

The F-statistic value is 11.39, which following the F distribution with df 5 and 78 under H0.
The p-value is very close to 0, so we can reject H0. Thus this model is meaningful.

Next, let's see the anova table.
First hypotheses are

H0: all $\gamma_i$ are 0.

H1: at least one $\gamma_i$ is not 0.

The F value of 'group:age' is 3.5402, which follows the F-distribution with df 2, 78 under H0.
The p-value is 0.03376. If we use a significance level 0.01, we cannot reject H0. But using 0.05, we can.

Next,

H0: all $\alpha_i$ are 0.

H1: at least one $\alpha_i$ is not 0.

The F value of 'group' is 3.8912, which follows the F-distribution with df 2, 78 under H0.
The p-value is 0.02450.
Again, if we use a significance level 0.05, we can reject the H0. But using 0.01, we cannot reject it.

Finally,

H0: $\beta = 0$

H1: $\beta \neq 0$

The F-value of age is 42.0915, which follows the F-distribution with df 1, 78 under H0.
The p-value is nearly 0, so we can reject H0.


So, if we use 0.01 significance level, we can reduce the model to simpler one like
\[y_{j} = \mu + \beta x_{ij} + \epsilon\]
for $j=1,...,84$. But I'll skip this work.


To sum up the fitting coefficients, for group 1,
\[\hat{y_{1j}} = 8.1834 - 0.0851 x_{1j}\]
and for group 2,
\[\hat{y_{2j}} = 8.1834 - 1.9534 + (-0.0851 + 0.0386) x_{2j}\]
and for group 3,
\[\hat{y_{3j}} = 8.1834 - 2.5031 + (-0.0851 + 0.0545) x_{3j}\]

```{r}
par(mfrow = c(1, 1))
plot(vital.capacity ~ age, data = vital, pch = 19, col = colors[group])
legend("bottomleft", pch = 19, col = colors,
  legend = c("group 1", "group 2", "group 3"))
vital_coeff <- vital_lm_fit$coefficients
vital_coeff
abline(vital_coeff[1], vital_coeff[4], col = colors[1])
abline(vital_coeff[1]+vital_coeff[2], vital_coeff[4]+vital_coeff[5], col = colors[2])
abline(vital_coeff[1]+vital_coeff[3], vital_coeff[4]+vital_coeff[6], col = colors[3])
```

Finally, here are the residual plot and the normal q-q plot.

```{r}
par(mfrow = c(2, 2))
plot(vital_lm_fit)
```

The residual plot seems fine.
In the normal q-q plot, we can find that points on the right tail part are not on the straight line.
This appears to violate the normal distribution assumption weakly.



### \noindent Problem5.
**An experiment was run to investigate the amount of weight loss (in grams)**
**by ground beef hamburgers after grilling or frying, and**
**how much the weight loss is affected by the percentage of fat in the beef before cooking.**
**The experiment involved 2 factors:**

- cooking method (with two levels, frying and grilling)
- fat content (with 3 levels: 10%, 15% and 20%)

**Hamburger patties weighing 110g each were prepared from meat with the required fat content.**
**There were 30 cooking time slots which were randomly assigned to the treatments**
**in such a way that each treatment was observed 5 times.**
**The patty weighs after cooking are shown below:**

- Method Frying, Fat Content 10%: 81, 88, 85, 84, 84
- Method Frying, Fat Content 15%: 85, 80, 82, 80, 82
- Method Frying, Fat Content 20%: 71, 77, 72, 80, 80
- Method Grilling, Fat Content 10%: 84, 84, 82, 81, 86
- Method Grilling, Fat Content 15%: 83, 88, 85, 86, 88
- Method Grilling, Fat Content 20%: 78, 75, 78, 79, 82 

```{r}
method <- factor(rep(c("frying", "grilling"), each = 5 * 3))
fat <- factor(rep(c("10%", "15%", "20%"), each = 5, times = 2))
weightloss <- c(81, 88, 85, 84, 84, 85, 80, 82, 80, 82, 71, 77, 72, 80, 80,
  84, 84, 82, 81, 86, 83, 88, 85, 86, 88, 78, 75, 78, 79, 82)
wl_data <- data.frame(method, fat, weightloss)
```

**1. Perform EDA.**

To begin with, let's draw boxplots.

```{r}
par(mfrow = c(1, 2))
boxplot(weightloss ~ method, data = wl_data)
boxplot(weightloss ~ fat, data = wl_data)
par(mfrow = c(1, 1))
boxplot(weightloss ~ method + fat, data = wl_data,
  las = 2, cex.axis = 0.8, xlab = "")
```

The difference between the two methods seems small.
However, in the case of the fat, the level '20%' is very different from the other two levels.
The box of 20% even does not overlap the other two boxes.

```{r}
par(mfrow = c(1, 2))
with(data = wl_data, expr = {
interaction.plot(method, fat, response = weightloss)
interaction.plot(fat, method, response = weightloss)
})
```

In interaction plots, lines are not parallel.
It implies there may or may not be an interaction between 'method' and 'fat'.


**2. Perform a two-way analysis of variance.**

**Explain your model and your conclusions. In particular:**

**- Compare the effects of the 3 levels of fat (taking into account cooking methods if needed) and interpret your results**

**- Give a 90% confidence interval for the mean difference in weight after cooking between frying and grilling 110g hamburgers.**

Now, our model is
\[y_{ijk} = \mu + \alpha_i + \beta_j + \gamma_{ij} + \epsilon_{ijk}\]
for $i=1,2$, $j=1,2,3$, and $k=1,2,3,4,5$, 
where

- y_{ijk}: amount of weight loss of i-th method(frying, grilling), j-th fat level (10%, 15%, 20%), and k-th observations
- $\alpha_i$: i-th method effect
- $\beta_j$: j-th fat level effect
- $\gamma_{ij}$: interaction effect between i-th method and j-th fat level

and $\epsilon_{ijk}$ are iid normal random variable with mean 0 and variance $\sigma^2$.

```{r}
wl_aov_fit <- aov(weightloss ~ method * fat, data = wl_data)
summary(wl_aov_fit)
```

The first test is for the interaction term.

H0 : all $\gamma_{ij}$ are 0.

H1 : at least one $\gamma_{ij}$ is not 0.

The F-value is 2.399, following the F-distribution with df 2 and 24.
We cannot reject H0 under common significance levels like 0.05 or 0.01,
because the p-value (0.112) is larger than these significance levels.
So, we cannot conclude that the interaction term is statistically significant.

Thus, let's reduce the model to the simpler one.

Now, our model becomes

\[y_{ijk} = \mu + \alpha_i + \beta_j + \epsilon_{ijk}\]
for $i=1,2$, $j=1,2,3$, and $k=1,2,3,4,5$, 
under the same notation and assumption.

The fitting result for the new model is here.
```{r}
wl_aov_fit2 <- aov(weightloss ~ method + fat, data = wl_data)
summary(wl_aov_fit2)
```

First Hypotheses are

H0: All $\alpha_i$ are 0.

H1: At least one $\alpha_i$ is 0. (or, $\alpha_2 \neq 0$)

The F-value of the 'method' is 3.247, which follows the F-distribution with df 1 and 26.
The p-value is 0.0832, so we cannot reject H0 under a common significance level like 0.05 or 0.01.
In other words, we cannot conclude that there is a difference between two cooking methods.

Second Hypotheses are

H0: All $\beta_j$ are 0.

H1: At least one $\beta_j$ is not 0.

The F-value of the 'fat' is 18.591, following the F-distribution with df 2 and 26.
The p-value is so close to 0, so we can reject the H0.
This means that there is a significant difference on the amount of weight loss among fat levels.


Next, the next table shows fitted mean differences from the grand mean by each level of fat.

```{r}
model.tables(wl_aov_fit2, cterms = "fat")
```

And, this table shows the mean values for each level of fat.

```{r}
model.tables(wl_aov_fit2, cterms = "fat", type = "mean")
```

To see how they differ, I will use Tukey's significant difference method.
Following the problem's direction, I use 90% confidence level for this work.

```{r}
wl_tukeydiff <- TukeyHSD(wl_aov_fit2, which = 2, conf = 0.9) #for fat
wl_tukeydiff
par(mfrow = c(1, 1))
plot(wl_tukeydiff)
```

From this R output, we can see the 90% confidence interval for the mean difference 
in weight among three fat levels.
Between the fat levels 10% and 20%, and between the levels 15% group and 20% group,
we can find significant differences in their average.

In addition, here are 90% CIs between two cooking methods, which are requested in the problem.

```{r}
wl_tukeydiff2 <- TukeyHSD(wl_aov_fit2, which = 1, conf = 0.9) #for method
wl_tukeydiff2
par(mfrow = c(1, 1))
plot(wl_tukeydiff2)
```

This 90% CI does not include zero.
This is a consistent result because the p-value of the F test about 'method' is 0.070.
Under significance level 0.1, we can reject H0.
(Note that the tests described in the previous paragraph used different significance levels, 0.05 or 0.01,
so I could not reject H0.)

Finally, for model diagnosis, let's draw the residual plot and the normal q-q plot.

```{r}
par(mfrow = c(2, 2))
plot(wl_aov_fit2)
```

All things look fine. We cannot find an outlier nor a pattern in the residual plot.
Points on the normal q-q plot seem straight enough.


### \noindent Problem6.
**In the ISwR malaria dataset, analyze the risk of malaria via logistic regression**
**with age and the log-transformed antibody level as explanatory variables.**
**Explain your model, analysis and conclusions.**

```{r}
library(ISwR)
str(malaria)
# ?malaria
summary(malaria)
```

There are 100 data points and 4 variables.
Each variable is

- subject: subject code.
- age: age in years.
- ab: antibody level.
- mal: a numeric vector code, Malaria: 0: no, 1: yes.


Let's look at the data.

```{r}
par(mfrow = c(2, 1))
plot(jitter(mal, 0.5) ~ jitter(age, 0.5), pch = 19, data = malaria,
  xlim = c(-1, 10), ylab = "malaria", xlab = "age")
plot(jitter(mal, 0.15) ~ log(ab), pch = 19, data = malaria, 
  xlim = c(-1, 10), ylab = "malaria", xlab = "log(antibody level)")
```
```{r}
par(mfrow = c(1, 1))
colors <- c("red", "blue")
plot(log(ab) ~ age, col = colors[mal + 1], pch = 19, data = malaria)
legend("bottomleft", pch = 19, col = colors,
  legend = c("mal-no", "mal-yes"))
```

It's hard to find a relation between age and malaria.
However, it seems that someone who has a higher log-transformed antibody level is less likely to have malaria.
Note that there is no explanatory variable of complete separation.


Let's set a model about the probability of having malaria using other two variables, 'age' and log transformed 'ab'.
Now, our model is
\[logit(P(y_i = 1 | x_{i1}, x_{i2})) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}\]
for i = 1,2,...,100, where

- $y_i$: 1 if i-th sample has malaria, 0 otherwise
- $x_{i1}$: age of i-th sample
- $x_{i2}$: log transformed antibody level of i-th sample

```{r}
mal_glm_fit <- glm(mal ~ age + log(ab), data = malaria, family = "binomial")
summary(mal_glm_fit)
```

To start with, let's check whether this model is significant or not.

H0 : $\beta_j$ ($j=1,2$) are 0.

H1 : at least one $\beta_j$ ($j=1,2$) is not 0.

```{r}
116.652 - 98.017 #chi-sq value
1 - pchisq(18.635, 2) #p-value
```

The difference of the two deviances is 19.635, which is following the chi-square distribution with df 2 under H0.
The p-value is very close to 0. We can reject H0 under common significance levels like 0.05 or 0.01.
Thus this model is statistically significant.

Second, let's see each coefficient.
Age is not significant, if we consider the z-test.
Note that according to Agresti's book, a wald type test may be problematic
for a logistic regression result.
But this case, the age variable may have little role 
when we consider the scatterplot in the above EDA procedure.
So, I think we can exclude the 'age' variable in our model.


Let's set another model about the probability of having malaria using only one variable,
log-transformed 'ab'.
Now, our model is
\[logit(P(y_i = 1 | x_{i1}, x_{i2})) = \alpha_0 + \alpha_1 x_{i}\]
for i = 1,2,...,100, where

- $y_i$: 1 if i-th sample has malaria, 0 otherwise
- $x_i$: log transformed antibody level of i-th sample

```{r}
mal_glm_fit2 <- glm(mal ~ log(ab), data = malaria, family = "binomial")
summary(mal_glm_fit2)
```

Using deviance values, let's test the new model's significance.

H0 : $\alpha_1 = 0$

H1 : $\alpha_1 \neq 0$

```{r}
116.652 - 98.968 # chi-square value
1 - pchisq(17.684, 1) #p-value
```

The difference between the two deviances is 17.684, which is following the chi-square distribution with df 1 under H0.
The p-value is very close to 0. We can reject H0 under common significance levels like 0.05 or 0.01.
Thus this model is statistically significant, and $\alpha_1$ is not 0.

The estimated $\alpha_1$ is -0.7122.
If log-transformed antibody level increase 1 unit,
the odd is multiplied by $e^{-0.7122}$.
Or equivalently, the odd ratio is
\[\frac{P(Y=1|X=c+1)/P(Y=0|X=c+1)}{P(Y=1|X=c)/P(Y=0|X=c)} = e^{-0.7122}\]

For example,
```{r}
logit_inv <- function(z){
  return(exp(z) / (1 + exp(z)))
}
mal_coeff <- mal_glm_fit2$coefficients
library(knitr)
kable(data.frame(log_ab = 0:7, probability = logit_inv(mal_coeff[1] + mal_coeff[2] * (0:7))))
```

We can draw the fitted curve.

```{r}
par(mfrow = c(1, 1))
plot(jitter(mal, 0.15) ~ log(ab), pch = 19, data = malaria,
  xlim = c(-1, 10), ylab = "malaria", xlab="log(antibody level)")
curve(logit_inv(mal_coeff[1] + mal_coeff[2] * x), add = TRUE, col = "red")
```

Let's evaluate this fitting result.

```{r}
mal_pred_prob <- predict(mal_glm_fit2, data.frame(ab = malaria$ab), type = "response")
mal_pred_binary <- ifelse(mal_pred_prob > 0.5, 1, 0)
sum(malaria$mal == mal_pred_binary)
sum(malaria$mal == 1 & mal_pred_binary == 1) / sum(malaria$mal == 1) #sensitivity
sum(malaria$mal == 0 & mal_pred_binary == 0) / sum(malaria$mal == 0) #specificity
```
If we choose the cutoff point as 0.5, the 72% points are correctly predicted.
But sensitivity $P(\hat{y}=1 | y=1)$ is low.

```{r}
par(mfrow = c(1, 1))
plot(0, 0, type = "n", xlim = c(0, 1), ylim = c(0, 1),
  xlab = "false positive", ylab = "sensitivity", main="ROC curve")
abline(0, 1, lty=2)

log_ab_df <- data.frame(ab = malaria$ab)

for(thres in seq(0, 1, by = 0.005)) {
  mal_pred_prob <- predict(mal_glm_fit2, log_ab_df, type = "response")
  mal_pred_binary <- ifelse(mal_pred_prob > thres, 1, 0)
  #data points of malaria data == 100
  sens_ratio <- sum(malaria$mal == 1 & mal_pred_binary == 1) / sum(malaria$mal == 1)
  falsepos_ratio <- sum(malaria$mal == 0 & mal_pred_binary == 1) / sum(malaria$mal == 0)

  points(falsepos_ratio, sens_ratio, pch = 19)
}
```

The ROC curve seems good.
Note that I drew this plot by re-using the training dataset.
I did not separate a training dataset and a testing dataset.
This may be a limitation in evaluating the predictive power of this model
by looking at the above ROC curve.
However, for now, I skip the cross-validation procedure.

Finally, I attach the diagnostic plots without explanation.
(Residual analysis for logistic regression models hasn't been covered in class yet.)

```{r}
par(mfrow = c(2, 2))
plot(mal_glm_fit2)
```
