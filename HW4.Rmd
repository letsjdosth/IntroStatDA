---
title: "Homework Assignment 4"
author: "Seokjun Choi"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

### \noindent Problem1.
**Albert and Rizzo, Chapter 9, problem 1**

**9.1 (Rounding first base). The data in “rounding.txt” gives the times**
**required to round first base for 22 baseball players using three styles: rounding**
**out, a narrow angle and a wide angle. The goal is to determine if the method**
**of rounding first base has a significant effect on times to round first base.**
**The data and the format of the data can be viewed using a text editor or**
**a spreadsheet. With the data file in the current working directory, input the**
**data using**

```{r}
rounding <- read.table("RX-data/rounding.txt", header = TRUE)
head(rounding)
```

**Check using the str function that the data is in stacked format with three**
**variables: time, method, player, where time is numeric and method and**
**player are factors.**

```{r}
str(rounding)
rounding$method <- factor(rounding$method)
rounding$block <- factor(rounding$block)
str(rounding)

par(mfrow = c(1, 1))
boxplot(times ~ method, data = rounding) #EDA
```

**Analyze the data assuming a randomized block design with time to round**
**first base as the response, method of rounding first as the treatment, and**
**player as the block variable. Plot residuals to check whether the assumptions**
**for the distribution of random error appear to be valid.**

Let $i=1,2,3$ be indices for methods and $j=1,2,...,22$ be indices for blocks (or, player).
Note that there is only one data point for each $(i,j)$. So I omit the cell-wise(?) index.
Then, the model becomes
\[y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}\]
where $\alpha_i$ are the effects of 'method', and $\beta_j$ are the block effects.
and $\epsilon_{ij}$ are iid, normally distributed errors with mean 0 and common variance $\sigma^2$.

```{r}
rounding_aov_fit <- aov(times ~ method + block, data = rounding)
summary(rounding_aov_fit)
```

Let's test whether the effects of 'method' and effects of blocks are significant or not.
hypotheses are

H0: all $\alpha_i$ are 0. (the method-effects are not significant on average.)

H1: at least one $\alpha_i$ is not 0. (the method-effects are significant on average.)

and

H0: all $\beta_i$ are 0. (the block effects are not significant.)

H1: at least one $\beta_i$ is not 0. (the block effects are significant.)

The anova table shows F values and p-values for each test.
For the former one, F value is 6.288, and it follows the F-distirbution with degrees of freedom 2 and 42 under H0.
The p-value is 0.00408, so se can reject the H0 with common significance levels like 0.01 or 0.05.
Thus, we can say that the method-effects are significant.

For the latter test, we can find that the F value is 26.970 which is pretty high, and under H0 it follows the
F-distribution with df 21 and 42. The p-value is very close to 0, so we can reject the H0 with ordinary significance level.
Therefore, we can say that the block effects are also significant.

Next, let's see the estimated values for the effects of 'methods'.
(We do not have an interest in interpreting the block effect.)
The upper table shows the estimated difference between means of each method and grand mean.
The lower table is about the mean values themselves.

```{r}
model.tables(rounding_aov_fit, cterms = "method")
model.tables(rounding_aov_fit, cterms = "method", type = "mean")
```

To see the aspect of the difference, let's use the tukey's significant difference method.

```{r}
rounding_tukeydiff <- TukeyHSD(rounding_aov_fit, which = 1) #only 'method'
rounding_tukeydiff
par(mfrow = c(1, 1))
plot(rounding_tukeydiff)
```

'WidAngle' is different significantly from other two methods.

Finally, for model diagnosis, let's draw the residual plot and the normal q-q plot.

```{r}
par(mfrow = c(2, 2))
plot(rounding_aov_fit)
```

In the residual plot, we can find a point that has high-residual value (7-th).
Not only that, the residuals have shape like a diamond. 
So we have to check whether the homogeneous variance assumption is valid or not.

In addition, in the normal q-q plot, tail parts of both two direction are not on a line.
It is questionable whether that the normal assumption is satisfied.
Especially, the 7-th data point is problematic. We need to act on that point.



### \noindent Problem2.
**Albert and Rizzo, Chapter 9, problem 2**

**9.2 (Speed of light). The morley data in R contains the classical data**
**of Michaelson and Morley on the speed of light, recording five experiments**
**of 20 consecutive runs each. The response is the speed of light measurement Speed.**
**The experiment is Expt and the run is Run. See the documentation (?morley)**
**and also http://lib.stat.cmu.edu/DASL/Stories/SpeedofLight.html**
**for more details about the experiments and the dataset.**

```{r}
head(morley)
# ?morley
```

According to the R help file, three variables are
- Expt: The experiment number, from 1 to 5.
- Run: The run number within each experiment.
- Speed: Speed-of-light measurement.

**Use the str function to check that there are 100 observations of the response**
**Speed, Expt, and Run; all integer variables.**

```{r}
str(morley)
```

**Convert Expt and Run to factors using**

```{r}
morley$Expt <- factor(morley$Expt)
morley$Run <- factor(morley$Run)
str(morley)
```


**Display a boxplot of Speed by Expt.**
**Speed of light is a constant, so we see there are some problems**
**because the measurements of speed do not seem to be consistent across the five experiments.**

```{r}
boxplot(Speed ~ Expt, data = morley)
```

**The data can be viewed as a randomized block experiment.**
**What is the null hypothesis of interest?**
**Analyze the data and residuals and summarize your conclusions.**

Let's test whether the measurements of light speed are consistent or not across the five experiments.
Because there is another factor, 'run', I will use it as a block.

Let $i=1,...,5$ be indices for the experiments and $j=1,2,...,20$ be indices for the run numbers.
Again, these dataset has one data point for each $(i,j)$. So it's not needed to use a subscript within each cell.

Then, the model becomes
\[y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}\]
where $\alpha_i$ are the experiment effects, and $\beta_j$ are the block (run) effects.
and $\epsilon_{ij}$ are iid, normally distributed errors with mean 0 and common variance $\sigma^2$.

Next, our hypothesis are
H0 : $\alpha_i=0$ for all $i$
(the measurements of light speed are consistent across the five experiments)

H1 : at least $\alpha_i$ is not 0.
(the measurements of light speed are not consistent across the five experiments)

```{r}
morley_aov_fit <- aov(Speed ~ Expt + Run, data = morley)
summary(morley_aov_fit)
```

The block effects are not significant. So, let's delete the variable. Then,
```{r}
morley_aov_fit2 <- aov(Speed ~ Expt, data = morley)
summary(morley_aov_fit2)
```

The F-value is 4.288, and it follows the F distribution with df 4 and 95.
The p-value is 0.00311, so we can reject H0 under significance level higher than 0.00311, like 0.01 or 0.05.
Thus, we cannot say that the measurements of light speed are not consistent across the five experiments.

Here are differences from grand mean, and mean values of the experiment groups.
```{r}
model.tables(morley_aov_fit2, cterms = "Expt")
model.tables(morley_aov_fit2, cterms = "Expt", type = "mean")
```

And, tukey's difference test says that there are differences between experiment 1 and 4, and between 1 and 5.
```{r}
morley_tukeydiff <- TukeyHSD(morley_aov_fit2, which = 1)
morley_tukeydiff
par(mfrow = c(1, 1))
plot(morley_tukeydiff)
```

Finally, here are the residual plot and the normal q-q plot.

```{r}
par(mfrow = c(2, 2))
plot(morley_aov_fit2)
```

Except for two or three points that seem outliers, they looks fine.
We cannot find any pattern from the residual plot, 
and almost all points are making a line in the q-q plot (if we delete the three points).


### \noindent Problem3.
**The data frame tb.dilute is available in the ISwR package.**
**Perform a two-way analysis of variance of these data.**
**Explain your model, analysis and conclusions.**

```{r}
library(ISwR)
tb <- tb.dilute
# ?tb.dilute
head(tb)
is.factor(tb$animal)
is.factor(tb$logdose)
```
According to the tb.dilute's R help file, 
this dataset consists of 18 data points, with three variables.
Each variable is
- reaction : a numeric vector, reaction sizes (average of diameters) for tuberculin skin pricks.
- animal: a factor with levels 1–6.
- logdose: a factor with levels 0.5, 0, and -0.5.

```{r}
par(mfrow = c(1, 2))
boxplot(reaction ~ animal, data = tb) #EDA
boxplot(reaction ~ logdose, data = tb)
```

we can find that the reaction sizes increase as the animal factor's values increase, and
the reaction sizes decrease as the logdose factor's values decrease.


When considering the type of each variable, a natural model is

\[y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}\]
for $i=1,2,...6, j=1,2,3$,
where $y_{ij}$ are reaction sizes of i-th animal factor and j-th logdose factor,
and $\alpha_i$ are i-th animal factor effect, $\beta_j$ are j-th logdose factor effect
(0.5, 0, -0.5, respectively),
and $\epsilon_{ij}$ are iid error, following $N(0, \sigma^2)$.

Note that we cannot add an interaction term, because there is only one data point
for each (i,j)-combination. If we add the term, the degrees of freedom of residual would be 0.

Let's fit the model.
```{r}
tb_aov_fit <- aov(reaction ~ animal + logdose, data = tb)
summary(tb_aov_fit)
```

To check whether each effect is significant or not, we can do tests.
Specifically, for $\alpha_i$,

H0: all $\alpha_i$ are 0. (There is no effect of animal factor on average.)

H1: there is at least different $\alpha_i$. (The effect of animal factor is significant on average.)

and for $\beta_j$,

H0: all $\beta_j$ are 0. (There is no effect of logdose factor on average.)

H1: there is at least different $\beta_j$. (The effect of logdose factor is significant on average.)

Let's see the above anova table that R gives to us
The F-value of the first test is 8.264, which following the F-distribution with df 5 and 10 under H0.
The p-value is 0.00253, so we can reject the H0 with significance level larger than 0.00253, like 0.05 or 0.01.
Thus, we can say that the effect of animal factor is significant.

Next, in case of the $\beta_j$'s test,
the F-value is 42.482, which following the F-distribution with df 2 and 10 under H0.
The p-value is close to 0, so we can reject the H0 with a common significance level.
Therefore, we can conclude that the effect of logdose factor is also significant.


Now, using fitted values and Tukey’s multiple comparison method, let's compare group means.

```{r}
model.tables(tb_aov_fit, type = "means")
tb_tukeydiff <- TukeyHSD(tb_aov_fit, which = c("animal", "logdose"))
tb_tukeydiff
par(mfrow = c(1, 2))
plot(tb_tukeydiff, las = 1)
```

Three pairs of animal factors (5-1, 6-1, 6-2) are different.
Not only that, two pairs of logdose factor (between -0.5 and 0.5, and between -0.5 and 0)
does not include 0 in their CI.

Finally, here are the residual plot and the normal q-q plot.
```{r}
par(mfrow = c(2, 2))
plot(tb_aov_fit)
```

In the both plots, we can find two outliers (13th and 14th points).
Even if we talk except for those two points, there are other problems.
We can find that a weak U-shape pattern in the residual plot.
And we can see that data points are not perfectly on the line in the normal q-q plot,

But depending on point of view, some other researcher may not think
that it is terribly problematic except for two outliers.
The U-shape is very weak, and the data points are 'nearly' linear in the q-q plot.
It is also known that ANOVA is quite robust!
So, I guess someone might say it's okay. It's a bit vague situation.




### \noindent Problem4.
Analyze the vitcap2 dataset in the ISwR package using analysis of covariance.
Explain your model, analysis and conclusions.

### \noindent Problem5.
An experiment was run to investigate the amount of weight loss (in grams)
by ground beef hamburgers after grilling or frying, and
how much the weight loss is affected by the percentage of fat in the beef before cooking.
The experiment involved 2 factors:
cooking method (with two levels, frying and grilling) and
fat content (with 3 levels: 10%, 15% and 20%).
Hamburger patties weighing 110g each were prepared from meat with the required fat content.
There were 30 cooking time slots which were randomly assigned to the treatments 
in such a way that each treatment was observed 5 times. 
The patty weighs after cooking are shown below:

- Method Frying, Fat Content 10%: 81, 88, 85, 84, 84
- Method Frying, Fat Content 15%: 85, 80, 82, 80, 82
- Method Frying, Fat Content 20%: 71, 77, 72, 80, 80
- Method Grilling, Fat Content 10%: 84, 84, 82, 81, 86
- Method Grilling, Fat Content 15%: 83, 88, 85, 86, 88
- Method Grilling, Fat Content 20%: 78, 75, 78, 79, 82 


1. Perform EDA.

2. Perform a two-way analysis of variance. 
Explain your model and your conclusions. In particular: 
- Compare the effects of the 3 levels of fat 
(taking into account cooking methods if needed) and interpret your results; 
- Give a 90% confidence interval for the mean difference in weight 
after cooking between frying and grilling 110g hamburgers. 


### \noindent Problem6.
In the ISwR malaria dataset, analyze the risk of malaria via logistic regression 
with age and the log-transformed antibody level as explanatory variables. 
Explain your model, analysis and conclusions.